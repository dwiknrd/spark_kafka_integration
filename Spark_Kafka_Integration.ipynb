{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"padding: 60px;\n",
    "  text-align: center;\n",
    "  background: #d4afb9;\n",
    "  color: #003049;\n",
    "  font-size: 20px;\">\n",
    "  <h2>Inclass: Spark Integration for Streaming Data</h2>\n",
    "   <hr>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f986ae13",
   "metadata": {},
   "source": [
    "# 🌟 Introduction to Data Streaming  \n",
    "\n",
    "📈 **Real-time data** adalah kunci di dunia keuangan yang bergerak cepat. **Data streaming** memungkinkan lembaga keuangan menganalisis data besar seperti harga saham, tren pasar, atau transaksi secara langsung. Dengan ini, keputusan dapat diambil cepat, risiko dimonitor, dan kecurangan dideteksi.  \n",
    "\n",
    "🎯 **Contoh Penggunaan Data Streaming**:  \n",
    "\n",
    "- 💹 **Analisis Pasar Saham**: Memantau harga saham dan membuat keputusan trading secara real-time.  \n",
    "- 🚨 **Deteksi Kecurangan**: Identifikasi aktivitas mencurigakan untuk mengurangi risiko.  \n",
    "- 🛒 **Pantauan Transaksi Pelanggan**: Tawarkan layanan personal berdasarkan pola transaksi langsung.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b9141",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🚀 Spark Structured Streaming  \n",
    "\n",
    "**Spark Structured Streaming** adalah ekstensi dari Apache Spark yang memproses data **real-time** menggunakan API sederhana yang sama dengan batch processing.  \n",
    "\n",
    "✨ **Kenapa Pilih Spark Structured Streaming?**  \n",
    "- 🧑‍💻 **Skalabilitas**: Mampu menangani data dalam jumlah besar.  \n",
    "- 🛡️ **Toleransi Kesalahan**: Data aman dengan fitur checkpointing.  \n",
    "- ⏱️ **Analisis Real-Time**: Query data langsung dengan sintaks SQL-like.  \n",
    "- 🔄 **Kode Terpadu**: Gunakan kode yang sama untuk batch dan streaming.  \n",
    "\n",
    "💡 Spark Structured Streaming memproses data dalam **micro-batches**, yaitu membagi aliran data menjadi potongan kecil agar lebih mudah dikelola.  \n",
    "\n",
    "📘 Lihat panduan lengkapnya di [Spark Documentation](https://spark.apache.org/docs/3.5.1/structured-streaming-programming-guide.html).  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66cf7b9",
   "metadata": {},
   "source": [
    "## 🧠 Basic Concept  \n",
    "\n",
    "### 🔄 Stream Processing  \n",
    "\n",
    "🏦 Di industri keuangan, stream processing membantu menganalisis data sensitif waktu seperti pergerakan harga saham atau log transaksi pelanggan. Dengan **Structured Streaming**, respons cepat terhadap perubahan pasar atau deteksi fraud menjadi mungkin.  \n",
    "\n",
    "💡 Bayangkan data stream sebagai **“Input Table”**: Setiap data baru adalah baris baru dalam tabel. Bedanya, pada batch processing, semua data diproses sekaligus, sedangkan pada streaming, data diproses terus-menerus saat datang.  \n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Cara Kerja:  \n",
    "\n",
    "1. **Data sebagai Tabel**:  \n",
    "   Bayangkan data saham masuk setiap detik. Setiap harga baru jadi baris baru di tabel yang dapat langsung di-query.  \n",
    "\n",
    "2. **Micro-Batching**:  \n",
    "   Alih-alih memproses satu per satu, data dikumpulkan dalam batch kecil untuk efisiensi.  \n",
    "\n",
    "3. **Proses Real-Time**:  \n",
    "   Data langsung dianalisis, diubah, atau dihitung layaknya batch processing.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧾 Contoh Real-Time Table  \n",
    "\n",
    "![](assets/structured-streaming-model.png)\n",
    "\n",
    "1. **Input Table**:  \n",
    "   Kosong di awal, lalu tumbuh saat data baru masuk (contoh: harga saham).  \n",
    "\n",
    "2. **Proses**:  \n",
    "   - 📊 Hitung rata-rata pergerakan harga saham.  \n",
    "   - 📈 Analisis tren perubahan harga mendadak.  \n",
    "   - 🕒 Agregasi data dalam rentang waktu (contoh: rata-rata harga 5 menit terakhir).  \n",
    "\n",
    "3. **Result Table**:  \n",
    "   Hasil di-update terus ke tabel, layar, atau database untuk mendukung **pengambilan keputusan real-time**.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa61547",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><br>\n",
    "  <center><h2>🔄 Workflow Spark Structured Streaming  </h2></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e991d51",
   "metadata": {},
   "source": [
    "# 🔄 Workflow Spark Structured Streaming  \n",
    "\n",
    "## 💻 Contoh: Pantauan Transaksi Real-Time  \n",
    "\n",
    "💡 Bayangkan data transaksi pelanggan dalam format **JSON**. Kita bisa:  \n",
    "- Menghitung total transaksi per pelanggan.  \n",
    "- Mendeteksi pola perilaku secara langsung.  \n",
    "\n",
    "###  1️⃣ **Impor Library dan Buat `SparkSession`**  \n",
    "\n",
    "Untuk memulai:  \n",
    "1. 🛠️ Impor library:  \n",
    "   - `StructType` & `StructField`: Mendefinisikan skema JSON.  \n",
    "   - `from_json` & `col`: Parsing dan interaksi dengan data JSON.  \n",
    "   - `sum`: Untuk operasi agregasi.  \n",
    "2. 🚀 Buat `SparkSession`: Pusat operasi Spark, termasuk untuk Structured Streaming.  \n",
    "\n",
    "✨ **SparkSession** adalah langkah awal untuk memproses aliran data dengan Spark.  \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0dc86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import from_json, col, sum\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TransactionStreaming\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15ceaf",
   "metadata": {},
   "source": [
    "### 2️⃣ **Tentukan Schema Data**  \n",
    "\n",
    "Schema mendefinisikan struktur data yang akan diterima. Gunakan `StructType` dan `StructField`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764311d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the transaction data\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"transaction_amount\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6b438d",
   "metadata": {},
   "source": [
    "📋 **Penjelasan**:  \n",
    "   - **`customer_id`**: ID unik pelanggan (String).  \n",
    "   - **`transaction_amount`**: Jumlah transaksi (Double).  \n",
    "   - **Nullable (`True`)**: Kolom dapat berisi nilai kosong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92dfec3",
   "metadata": {},
   "source": [
    "### **3️⃣ Baca Data Streaming dengan Spark**\n",
    "\n",
    "Kita menggunakan **`readStream`** untuk membaca data dari sumber streaming. Dalam contoh ini, data real-time disimulasikan melalui **socket**.\n",
    "\n",
    "**🔌 Sumber Data: Socket**\n",
    "\n",
    "Socket digunakan untuk mengirim data real-time.  \n",
    "Konfigurasi yang diperlukan:  \n",
    "\n",
    "- **`host`**: Lokasi server data (contoh: `localhost`).  \n",
    "- **`port`**: Nomor port untuk koneksi (contoh: `9999`).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afba755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the stream of transaction data from a socket\n",
    "raw_stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b363cb",
   "metadata": {},
   "source": [
    "**📦 Parsing Data JSON**\n",
    "\n",
    "Data mentah dalam format JSON diubah menjadi kolom terstruktur dengan langkah berikut:  \n",
    "1. **`from_json`**: Mengonversi string JSON menjadi kolom terstruktur sesuai **`transaction_schema`**.  \n",
    "2. **`col(\"value\")`**: Mengakses data mentah di stream.  \n",
    "3. **`alias(\"data\")` + `select(\"data.*\")`**: Ekstrak kolom seperti **`customer_id`** dan **`transaction_amount`** untuk pemrosesan lebih mudah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the JSON data\n",
    "transaction_stream_df = raw_stream_df \\\n",
    "    .select(from_json(col(\"value\"), transaction_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb704929",
   "metadata": {},
   "source": [
    "### 4️⃣ **Proses Data**\n",
    "\n",
    "Setelah data streaming dibaca, langkah selanjutnya adalah **mengolah data**. Pada contoh ini, kita ingin:  \n",
    "\n",
    "- **Grouping**: Mengelompokkan data berdasarkan `customer_id` agar setiap pelanggan dapat dianalisis.  \n",
    "- **Aggregation**:\n",
    "  - **`sum(\"transaction_amount\")`**: Menjumlahkan total transaksi setiap pelanggan.  \n",
    "  - **`alias(\"total_amount\")`**: Memberi nama baru pada kolom hasil agregasi agar lebih jelas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81033a0",
   "metadata": {},
   "source": [
    "**📊 Hitung Total Transaksi per Pelanggan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e391ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total transaction amount for each customer\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "customer_total_df = transaction_stream_df \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(sum(\"transaction_amount\").alias(\"total_amount\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c33177",
   "metadata": {},
   "source": [
    "### 5️⃣ **Tampilkan Hasil**\n",
    "\n",
    "Setelah data diproses, hasilnya ditulis ke **console** agar bisa dimonitor.  \n",
    "\n",
    "- **`writeStream`**: Menentukan tujuan keluaran data hasil proses.  \n",
    "  - **`outputMode(\"complete\")`**: Menampilkan hasil agregasi penuh setiap kali query dijalankan.  \n",
    "  - **`format(\"console\")`**: Menampilkan hasil di terminal.  \n",
    "- **`start()`**: Memulai proses streaming secara real-time.  \n",
    "- **`awaitTermination()`**: Menjaga aplikasi tetap berjalan hingga dihentikan manual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5947a37",
   "metadata": {},
   "source": [
    "**📤 Kode untuk Menulis Hasil ke Console**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad404b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the result to the console\n",
    "query = customer_total_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the termination of the query\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796102a3",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "  <center>Kode ini akan error apabila langsung dijalankan tanpa menjalankan socket `nc` di terminal</center>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a00dc",
   "metadata": {},
   "source": [
    "### 6️⃣ **Simulasi Data Transaksi Real-Time**\n",
    "\n",
    "Untuk menguji, gunakan **`nc` (netcat)** untuk mengirim data JSON ke socket.  \n",
    "1. Jalankan perintah berikut di terminal:  \n",
    "   ```bash\n",
    "   nc -lk 9999\n",
    "   ```  \n",
    "   - **`-l`**: Mode listening untuk menerima koneksi.  \n",
    "   - **`-k`**: Membuka koneksi terus-menerus.  \n",
    "\n",
    "2. Masukkan data transaksi dalam format JSON:  \n",
    "   ```json\n",
    "   {\"customer_id\": \"C001\", \"transaction_amount\": 100.50}\n",
    "   {\"customer_id\": \"C002\", \"transaction_amount\": 200.75}\n",
    "   {\"customer_id\": \"C001\", \"transaction_amount\": 150.25}\n",
    "   {\"customer_id\": \"C003\", \"transaction_amount\": 50.00}\n",
    "   {\"customer_id\": \"C002\", \"transaction_amount\": 100.00}\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812a494",
   "metadata": {},
   "source": [
    "**🖥️ Hasil di Console**\n",
    "\n",
    "Spark akan memproses data dan menampilkan hasil agregasi secara real-time:\n",
    "\n",
    "```plaintext\n",
    "+-----------+------------+\n",
    "|customer_id|total_amount|\n",
    "+-----------+------------+\n",
    "|       C001|      250.75|\n",
    "|       C002|      300.75|\n",
    "|       C003|       50.00|\n",
    "+-----------+------------+\n",
    "```\n",
    "\n",
    "\n",
    "**📌 Penjelasan Hasil**\n",
    "- **`C001`**: 100.50 + 150.25 = **250.75**  \n",
    "- **`C002`**: 200.75 + 100.00 = **300.75**  \n",
    "- **`C003`**: Hanya 50.00.\n",
    "\n",
    "✨ Dengan simulasi ini, Anda bisa melihat bagaimana data streaming diproses secara real-time oleh Spark! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e84d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert\">\n",
    "\n",
    "**Why Netcat? 🤔**\n",
    "\n",
    "🔹 **Simulasi Mudah**: Uji aliran data real-time tanpa sistem kompleks.  \n",
    "🔹 **Cocok untuk Pemula**: Belajar konsep streaming sebelum masuk ke tools produksi.\n",
    "\n",
    "**Untuk Produksi** \n",
    "\n",
    "Ganti **Netcat** dengan sistem yang lebih andal, seperti: \n",
    " \n",
    "- **Apache Kafka**  \n",
    "- **Amazon Kinesis**  \n",
    "- **Google Pub/Sub**\n",
    "</div> \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3facc78b",
   "metadata": {},
   "source": [
    "# 🚀 Enhancing Real-Time Streaming with Messaging Systems  \n",
    "\n",
    "Dalam dunia keuangan, seperti **monitoring transaksi**, **deteksi penipuan**, atau **analisis harga saham**, kita perlu alat andal untuk menangani data real-time. Di sinilah **Apache Kafka** hadir! Berbeda dengan server sederhana seperti `nc`, Kafka adalah platform terdistribusi yang dirancang khusus untuk mengelola data aliran besar secara efisien. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4bdb32",
   "metadata": {},
   "source": [
    "## 🧐 Apa itu Apache Kafka?  \n",
    "\n",
    "**Apache Kafka** adalah sistem pesan terdistribusi open-source untuk menangani aliran data secara real-time. Dibuat oleh LinkedIn, Kafka kini menjadi alat penting dalam industri, termasuk keuangan, untuk mengolah data dengan cepat dan skalabel.  \n",
    "\n",
    "### 🌟 *Kenapa Kafka Penting untuk Keuangan?*  \n",
    "Kafka membantu bisnis:  \n",
    "1. 💸 **Memproses Transaksi**: Tangani jutaan transaksi per detik dengan latensi rendah.  \n",
    "2. 🔒 **Deteksi Penipuan**: Identifikasi pola mencurigakan secara real-time.  \n",
    "3. 📈 **Analisis Saham**: Pantau data pasar langsung untuk keputusan cepat.  \n",
    "4. 👥 **Wawasan Pelanggan**: Personalisasi layanan berdasarkan data perilaku real-time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b59831",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🧩 Komponen Utama Kafka  \n",
    "\n",
    "Mari kita kenali bagian-bagian inti Kafka:  \n",
    "\n",
    "1. **📌 Topics**:  \n",
    "   - Saluran untuk pesan, seperti `\"transactions\"` atau `\"stock_prices\"`.  \n",
    "   - Semua pesan terkait disimpan di topik untuk akses real-time.  \n",
    "\n",
    "2. **✉️ Producers**:  \n",
    "   - Sistem yang mengirim pesan ke topik Kafka. Contoh: Platform trading mengirim data ke `\"transactions\"`.  \n",
    "\n",
    "3. **🛠️ Consumers**:  \n",
    "   - Aplikasi yang membaca pesan dari topik. Misalnya, aplikasi deteksi penipuan membaca dari `\"transactions\"`.  \n",
    "\n",
    "4. **🔗 Brokers**:  \n",
    "   - Server yang menyimpan dan mengelola pesan. Kafka mendistribusikan dan mereplikasi data untuk ketahanan.  \n",
    "\n",
    "5. **⚙️ Partitions**:  \n",
    "   - Membagi topik menjadi bagian lebih kecil, misalnya berdasarkan wilayah: `\"Jakarta\"`, `\"Bogor\"`.  \n",
    "\n",
    "6. **🔢 Offset**:  \n",
    "   - ID unik untuk setiap pesan dalam partisi. Konsumen melacak pesan dengan offset ini.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5197a5d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔄 Bagaimana Kafka Mengelola Data Real-Time?  \n",
    "\n",
    "1. **📤 Mengirim Data ke Kafka**  \n",
    "   - **Producers** mengirim pesan ke topik. Contoh:  \n",
    "     ```json  \n",
    "     {\"transaction_id\": \"T12345\", \"amount\": 500.0, \"currency\": \"USD\"}  \n",
    "     ```  \n",
    "\n",
    "2. **📂 Partisi dan Penyimpanan**  \n",
    "   - Kafka mempartisi data berdasarkan kunci (misalnya ID transaksi).  \n",
    "   - Pesan disimpan berurutan untuk proses real-time.  \n",
    "\n",
    "3. **📥 Mengonsumsi Data**  \n",
    "   - **Consumers** membaca pesan dari topik dan memprosesnya (e.g., model deteksi penipuan).  \n",
    "\n",
    "4. **⏳ Penyimpanan dan Retensi**  \n",
    "   - Kafka menyimpan pesan untuk durasi tertentu, penting untuk audit & kepatuhan.  \n",
    "\n",
    "5. **📈 Skalabilitas dan Ketahanan**  \n",
    "   - Kafka tetap cepat meski volume data besar. Jika broker gagal, replika mengambil alih otomatis.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d348ad8",
   "metadata": {},
   "source": [
    "## 📈 Finance Use Case: Real-Time Loan Monitoring with Kafka  \n",
    "\n",
    "### **📝 Scenario**  \n",
    "\n",
    "Sebuah bank ingin memproses aplikasi pinjaman secara **real-time** untuk memberikan keputusan cepat: **disetujui** atau **ditolak**. Dengan **Apache Kafka**, bank dapat meningkatkan efisiensi dan keandalan proses ini. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **🔄 Alur Proses**  \n",
    "\n",
    "![](assets/loan_applications_kafka.png)\n",
    "\n",
    "\n",
    "#### **1️⃣ Producer: Mengirim Data Aplikasi Pinjaman**  \n",
    "\n",
    "- **Apa yang dilakukan**:  \n",
    "  Aplikasi perbankan online mengirim data pelanggan (misalnya pendapatan, skor kredit, jumlah pinjaman) ke topik Kafka bernama `\"loan_applications\"`.  \n",
    "\n",
    "\n",
    "#### **2️⃣ Broker: Mengelola Aliran Data**  \n",
    "\n",
    "- **Peran Kafka**:  \n",
    "  - Membagi topik `\"loan_applications\"` menjadi partisi (misalnya berdasarkan wilayah).  \n",
    "  - Mereplikasi data untuk memastikan ketersediaan tinggi, meskipun server gagal.  \n",
    "  - Menyimpan data untuk durasi tertentu guna keperluan reprocessing.  \n",
    "- **Manfaat**:  \n",
    "  Kafka memastikan **tidak ada data yang hilang** dan memungkinkan proses paralel untuk keputusan cepat.  \n",
    "\n",
    "#### **3️⃣ Consumer: Prediksi Keputusan Pinjaman**  \n",
    "\n",
    "- **Peran Consumer**:  \n",
    "  - Membaca data aplikasi dari topik Kafka.  \n",
    "  - **Model pembelajaran mesin** menganalisis fitur seperti pendapatan dan skor kredit untuk memutuskan:  \n",
    "    - ✅ **Disetujui**: Skor kredit tinggi dan pendapatan memadai.  \n",
    "    - ❌ **Ditolak**: Skor kredit rendah atau jumlah pinjaman terlalu besar.  \n",
    "- **Output**: Hasil dikirim ke topik baru, namun pada pembelajaran kali ini kita hanya menampilkan pada jupyter notebook console \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0090491b",
   "metadata": {},
   "source": [
    "## ⚙️ Langkah Persiapan Kafka  \n",
    "\n",
    "### **🖥️ Instalasi di Mac**  \n",
    "\n",
    "1. **📥 Unduh Kafka**  \n",
    "   - Kunjungi [Apache Kafka Download Page](https://kafka.apache.org/downloads).  \n",
    "   - Unduh versi binari Kafka (misalnya `kafka_2.13-3.8.1.tgz`).  \n",
    "\n",
    "2. **🗂️ Ekstraksi File Kafka**  \n",
    "   - Ekstrak file ke direktori seperti `Tools/kafka`.  \n",
    "\n",
    "3. **🔧 Konfigurasi Direktori Data Kafka**  \n",
    "   - Buat folder `data` di dalam direktori Kafka.  \n",
    "   - Edit file `config/kraft/server.properties` dan ubah:  \n",
    "     ```properties\n",
    "     log.dirs=data\n",
    "     ```\n",
    "   - Simpan perubahan.  \n",
    "\n",
    "4. **🆔 Format Penyimpanan Kafka**  \n",
    "   - Hasilkan UUID untuk cluster:  \n",
    "     ```bash\n",
    "     ./bin/kafka-storage.sh random-uuid\n",
    "     ```  \n",
    "   - Gunakan UUID ini untuk memformat penyimpanan Kafka:  \n",
    "     ```bash\n",
    "     ./bin/kafka-storage.sh format --cluster-id <UUID> --config config/kraft/server.properties\n",
    "     ```  \n",
    "\n",
    "5. **▶️ Menjalankan Kafka**  \n",
    "   - Mulai Kafka:  \n",
    "     ```bash\n",
    "     ./bin/kafka-server-start.sh config/kraft/server.properties\n",
    "     ```  \n",
    "   - Hentikan Kafka dengan `CTRL+C`.  \n",
    "\n",
    "---\n",
    "\n",
    "### **🖥️ Instalasi di Windows**  \n",
    "\n",
    "1. **📥 Unduh Kafka**  \n",
    "   - Unduh file Kafka dan ekstrak menggunakan **WinRAR** atau alat serupa.  \n",
    "   - Rename folder ke `kafka` untuk kemudahan.  \n",
    "\n",
    "2. **🔧 Konfigurasi ZooKeeper dan Kafka**  \n",
    "   - Ubah lokasi penyimpanan data di `config/zookeeper.properties` dan `config/kraft/server.properties`:  \n",
    "     ```properties\n",
    "     dataDir=data/zookeeper\n",
    "     log.dirs=data/kafka-logs\n",
    "     ```  \n",
    "\n",
    "3. **🆔 Format Penyimpanan Kafka**  \n",
    "   - Hasilkan UUID dengan:  \n",
    "     ```bash\n",
    "     bin\\windows\\kafka-storage.bat random-uuid\n",
    "     ```  \n",
    "   - Format penyimpanan Kafka:  \n",
    "     ```bash\n",
    "     bin\\windows\\kafka-storage.bat format --cluster-id <UUID> --config config\\kraft\\server.properties\n",
    "     ```  \n",
    "\n",
    "4. **▶️ Menjalankan Kafka**  \n",
    "   - Jalankan ZooKeeper:  \n",
    "     ```bash\n",
    "     bin\\windows\\zookeeper-server-start.bat config\\zookeeper.properties\n",
    "     ```  \n",
    "   - Jalankan Kafka:  \n",
    "     ```bash\n",
    "     bin\\windows\\kafka-server-start.bat config\\kraft\\server.properties\n",
    "     ```  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a287d7c",
   "metadata": {},
   "source": [
    "## Step 2: Membuat Kafka Producer\n",
    "\n",
    "![](assets/st_UI_producer.png)\n",
    "\n",
    "Kafka producer digunakan untuk mengirim pesan ke topik Kafka agar dapat diproses lebih lanjut. Dalam langkah ini, kita akan:  \n",
    "\n",
    "1. Mengkonfigurasi Kafka producer menggunakan pustaka `confluent_kafka`.  \n",
    "2. Membangun antarmuka pengguna (UI) dengan Streamlit untuk input data aplikasi pinjaman.  \n",
    "3. Mengirim data yang dikumpulkan ke topik Kafka bernama `\"loan_applications\"`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f6c7b",
   "metadata": {},
   "source": [
    "### **1️⃣ Kode Kafka Producer**\n",
    "\n",
    "Berikut adalah kode Python untuk mengatur Kafka producer:\n",
    "\n",
    "\n",
    "```python\n",
    "from confluent_kafka import Producer\n",
    "import json\n",
    "\n",
    "# Konfigurasi Kafka producer\n",
    "producer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092'  # Alamat Kafka broker\n",
    "}\n",
    "\n",
    "# Membuat instance Kafka producer\n",
    "producer = Producer(producer_config)\n",
    "\n",
    "def kirim_pesan(topik, nilai):\n",
    "    \"\"\"\n",
    "    Mengirim pesan ke topik Kafka yang ditentukan.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Kirim data ke topik Kafka dalam format JSON\n",
    "        producer.produce(topik, value=json.dumps(nilai))\n",
    "        producer.flush()  # Pastikan pesan terkirim\n",
    "        print(f\"Pesan berhasil dikirim ke topik '{topik}': {nilai}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Kesalahan saat mengirim pesan: {e}\")\n",
    "```\n",
    "\n",
    "**🔑 Poin Penting**:  \n",
    "\n",
    "- **`bootstrap.servers`**: Alamat Kafka broker. Secara default, menggunakan `localhost:9092` untuk pengaturan lokal.  \n",
    "- **`produce()`**: Mengirim data ke topik Kafka yang ditentukan.  \n",
    "- **`flush()`**: Memastikan semua pesan yang tersimpan dalam buffer terkirim sebelum melanjutkan proses.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a0dc9",
   "metadata": {},
   "source": [
    "### **2️⃣ Membuat Antarmuka dengan Streamlit**\n",
    "\n",
    "Berikut adalah cara membuat antarmuka web interaktif menggunakan Streamlit untuk pengumpulan data aplikasi pinjaman:\n",
    "\n",
    "```python\n",
    "# Streamlit UI for loan applications\n",
    "st.title(\"Credit Risk Prediction\")\n",
    "\n",
    "with st.form(\"credit_risk_form\"):\n",
    "    person_home_ownership = st.selectbox(\"Home Ownership\", [\"RENT\", \"OWN\", \"MORTGAGE\"])\n",
    "    loan_intent = st.selectbox(\"Loan Intent\", [\"PERSONAL\", \"EDUCATION\", \"DEBT CONSOLIDATION\"])\n",
    "    loan_grade = st.selectbox(\"Loan Grade\", [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"])\n",
    "    person_age = st.number_input(\"Age\", min_value=0, step=1)\n",
    "    person_income = st.number_input(\"Income\", min_value=0.0, step=0.1)\n",
    "    person_emp_length = st.number_input(\"Employment Length (in years)\", min_value=0, step=1)\n",
    "    loan_amnt = st.number_input(\"Loan Amount\", min_value=0.0, step=0.1)\n",
    "    loan_int_rate = st.number_input(\"Interest Rate\", min_value=0.0, step=0.1)\n",
    "    cb_person_cred_hist_length = st.number_input(\"Credit History Length (in years)\", min_value=0, step=1)\n",
    "\n",
    "    submitted = st.form_submit_button(\"Submit\")\n",
    "\n",
    "# Menangani pengiriman formulir\n",
    "if submitted:\n",
    "    # Mengumpulkan data dari formulir ke dalam dictionary\n",
    "    data_pinjaman = {\n",
    "        \"person_home_ownership\": person_home_ownership,\n",
    "        \"loan_intent\": loan_intent,\n",
    "        \"loan_grade\": loan_grade,\n",
    "        \"person_age\": person_age,\n",
    "        \"person_income\": person_income,\n",
    "        \"person_emp_length\": person_emp_length,\n",
    "        \"loan_amnt\": loan_amnt,\n",
    "        \"loan_int_rate\": loan_int_rate,\n",
    "        \"cb_person_cred_hist_length\": cb_person_cred_hist_length\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Mengirim data ke Kafka\n",
    "        kirim_pesan(\"loan_applications\", data_pinjaman)\n",
    "        st.success(\"✅ Aplikasi pinjaman berhasil dikirim ke Kafka!\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"❌ Kesalahan saat mengirim data ke Kafka: {e}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30851f9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### **How This Works:**\n",
    "\n",
    "- **Input Pengguna**:  \n",
    "  Pengguna memasukkan detail pinjaman, seperti **usia**, **pendapatan**, **jumlah pinjaman**, dll., ke dalam formulir.  \n",
    "\n",
    "- **Transformasi Data**:  \n",
    "  Data dari formulir dikumpulkan ke dalam dictionary dan diubah menjadi format JSON sebelum dikirim ke Kafka.  \n",
    "\n",
    "- **Pengiriman Pesan**:  \n",
    "  Kafka producer mempublikasikan pesan JSON ke topik `\"loan_applications\"`.  \n",
    "\n",
    "\n",
    "### 3️⃣ **Menjalankan Aplikasi Streamlit**\n",
    "\n",
    "1. Simpan kode ke file, misalnya `streamlit_kafka_producer.py`.  \n",
    "2. Jalankan aplikasi Streamlit:  \n",
    "   ```bash\n",
    "   streamlit run streamlit_kafka_producer.py\n",
    "   ```  \n",
    "3. Buka browser (Streamlit akan menampilkan tautannya di terminal). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b5a2d2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><br>\n",
    "  <center><h2>Read Data From Kafka</h2></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b27da5b",
   "metadata": {},
   "source": [
    "## Step 3: Kafka Consumer untuk Membaca Data dan Mengubahnya ke DataFrame\n",
    "\n",
    "### Membaca Data dari Kafka\n",
    "\n",
    "Pada langkah ini, kita akan membaca pesan yang dikirim oleh Kafka Producer dan mengubahnya ke format terstruktur (seperti DataFrame). Untuk itu, kita akan menggunakan **PySpark** sebagai alat untuk memproses dan menganalisis data Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed5520",
   "metadata": {},
   "source": [
    "**1️⃣ Mengatur PySpark dengan Kafka**\n",
    "\n",
    "Langkah pertama adalah membuat **Spark session** yang memungkinkan kita berinteraksi dengan Spark dan Kafka.\n",
    "\n",
    "Namun ada beberapa configurasi yang peru kita atur:\n",
    "\n",
    "- **`spark.jars.packages`**: Menentukan pustaka Kafka dan Spark SQL yang diperlukan untuk membaca data dari Kafka.\n",
    "- **`master(\"local[*]\")`**: Memberitahu Spark untuk berjalan secara lokal dengan memanfaatkan semua core CPU yang tersedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce2b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Credit Data Kafka\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,org.apache.kafka:kafka-clients:3.8.1\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()  # Create the session\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f30a7e6",
   "metadata": {},
   "source": [
    "**2️⃣ Membaca Data dari Kafka**\n",
    "\n",
    "Setelah Spark session dibuat, kita bisa membaca data dari topik Kafka menggunakan fungsi `spark.read.format(\"kafka\")`.\n",
    "\n",
    "Ada beberapa `option` (pengaturan) yang perlu didefinisikan:\n",
    "\n",
    "- **`kafka.bootstrap.servers`**: Alamat Kafka broker, dalam hal ini `localhost:9092` untuk server lokal.\n",
    "- **`subscribe`**: Nama topik Kafka yang akan dibaca, misalnya `loan_applications`.\n",
    "- **`startingOffsets`**: Mengatur agar pembacaan dimulai dari data awal (earliest) yang tersedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"kafka\")  # Gunakan Kafka sebagai sumber data\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")  # Alamat Kafka broker\n",
    "    .option(\"subscribe\", \"loan_applications\")  # Nama topik Kafka\n",
    "    .option(\"startingOffsets\", \"earliest\")  # Mulai membaca dari offset paling awal\n",
    "    .load()  # Memuat data ke dalam DataFrame\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f536c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan struktur data\n",
    "kafka_df.printSchema()  # Print schema to see the structure of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2263bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df.show()  # Display the first few rows of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7783ef",
   "metadata": {},
   "source": [
    "\n",
    "**3️⃣ Mendefinisikan Skema untuk Data**\n",
    "\n",
    "Pesan dari Kafka disimpan dalam format **binary**, sehingga kita perlu mengubahnya menjadi format terstruktur. Untuk itu, kita mendefinisikan **schema** yang sesuai dengan data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22eac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "# Define the schema for the data inside the 'value' column\n",
    "schema = StructType([\n",
    "    StructField(\"person_home_ownership\", StringType(), True),\n",
    "    StructField(\"loan_intent\", StringType(), True),\n",
    "    StructField(\"loan_grade\", StringType(), True),\n",
    "    StructField(\"person_age\", IntegerType(), True),\n",
    "    StructField(\"person_income\", FloatType(), True),\n",
    "    StructField(\"person_emp_length\", IntegerType(), True),\n",
    "    StructField(\"loan_amnt\", FloatType(), True),\n",
    "    StructField(\"loan_int_rate\", FloatType(), True),\n",
    "    StructField(\"cb_person_cred_hist_length\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef1967",
   "metadata": {},
   "source": [
    "- **`StringType`** digunakan untuk data teks (misalnya `person_home_ownership`).\n",
    "- **`IntegerType`** dan **`FloatType`** digunakan untuk data numerik (misalnya `person_age`, `loan_amnt`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6fef17",
   "metadata": {},
   "source": [
    "**4️⃣ Mengubah Pesan Kafka ke Format String dan Parsing JSON**\n",
    "\n",
    "Pesan Kafka dalam kolom **`value`** perlu diubah dari format binary menjadi string, lalu diparsing ke format JSON menggunakan schema yang sudah didefinisikan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ubah 'value' dari binary ke string dan parsing ke format JSON\n",
    "\n",
    "# Konversi binary ke string\n",
    "df = kafka_df.withColumn(\"value_str\", col(\"value\").cast(\"string\"))\n",
    "\n",
    "# Parsing JSON berdasarkan schema\n",
    "df_parsed = df.withColumn(\"data\", from_json(col(\"value_str\"), schema)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c3aa6",
   "metadata": {},
   "source": [
    "- **`col(\"value\").cast(\"string\")`**: Mengonversi data Kafka dari format binary ke string.\n",
    "- **`from_json(col(\"value_str\"), schema)`**: Memparsing string JSON ke DataFrame terstruktur berdasarkan schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada6c4be",
   "metadata": {},
   "source": [
    "**5️⃣ Memilih dan Menampilkan Data**\n",
    "\n",
    "Setelah parsing, kita dapat memilih kolom yang relevan dan menampilkan hasilnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memilih kolom yang relevan\n",
    "df_result = df_parsed.select(\n",
    "    \"data.*\",  # Semua kolom dari data yang diparsing\n",
    "    \"key\", \"topic\", \"partition\", \"offset\", \"timestamp\", \"timestampType\"  # Metadata Kafka\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0954c",
   "metadata": {},
   "source": [
    "- **`select(\"data.*\", ...)`**: This extracts all the fields from the parsed JSON data and adds Kafka metadata (like `key`, `topic`, `partition`, etc.).\n",
    "- **`df_result.show()`**: This displays the DataFrame content, allowing you to view the structured data from Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3957be74",
   "metadata": {},
   "source": [
    "**6️⃣ Hasil Akhir**\n",
    "\n",
    "Setelah kode di atas dijalankan, data yang diterima dari Kafka akan tampil dalam format terstruktur seperti berikut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83235bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan hasil\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79fa95",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\"><br>\n",
    "  <center><h2>Streaming Data From Kafka</h2></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd5d9e",
   "metadata": {},
   "source": [
    "## Streaming Data dari Kafka\n",
    "\n",
    "Pada langkah ini, kita akan menggunakan data **streaming** dari Kafka, yang memungkinkan kita membaca data secara real-time dan memprosesnya secara langsung. Tujuannya adalah membaca data dari topik Kafka dan mengubahnya menjadi format terstruktur menggunakan **PySpark**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156cf8b",
   "metadata": {},
   "source": [
    "**1️⃣ Mengatur Spark Session**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Credit Data Kafka\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,org.apache.kafka:kafka-clients:3.8.1\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()  # Create the session\n",
    "\n",
    "spark  # Display the Spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3f52e",
   "metadata": {},
   "source": [
    "**2️⃣ Membaca Data Streaming dari Kafka**\n",
    "\n",
    "Setelah Spark session dibuat, kita dapat menggunakan fungsi **`readStream`** untuk terus membaca pesan dari topik Kafka secara real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4462f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")  # Membaca data dari Kafka\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")  # Alamat server Kafka\n",
    "    .option(\"subscribe\", \"loan_applications\")  # Nama topik Kafka\n",
    "    .option(\"startingOffsets\", \"earliest\")  # Mulai membaca dari data awal\n",
    "    .load()  # Memuat data ke dalam DataFrame\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165d69c",
   "metadata": {},
   "source": [
    "**3️⃣ Mendefinisikan Skema untuk Data**\n",
    "\n",
    "Pesan dari Kafka biasanya berupa format **binary**, sehingga kita perlu mendefinisikan skema untuk mengubah data mentah menjadi format terstruktur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff841a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "# Definisi skema untuk kolom 'value'\n",
    "schema = StructType([\n",
    "    StructField(\"person_home_ownership\", StringType(), True),\n",
    "    StructField(\"loan_intent\", StringType(), True),\n",
    "    StructField(\"loan_grade\", StringType(), True),\n",
    "    StructField(\"person_age\", IntegerType(), True),\n",
    "    StructField(\"person_income\", FloatType(), True),\n",
    "    StructField(\"person_emp_length\", IntegerType(), True),\n",
    "    StructField(\"loan_amnt\", FloatType(), True),\n",
    "    StructField(\"loan_int_rate\", FloatType(), True),\n",
    "    StructField(\"cb_person_cred_hist_length\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132ed708",
   "metadata": {},
   "source": [
    "**4️⃣ Parsing Data dari Format Binary ke JSON**\n",
    "\n",
    "Setelah skema didefinisikan, kita dapat mengonversi kolom **`value`** (yang berisi data binary) menjadi string yang dapat dibaca. Kemudian, kita parsing data JSON menggunakan skema yang sudah dibuat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ubah 'value' dari binary ke string, lalu parsing ke format JSON\n",
    "df_parsed = stream_kafka_df.withColumn(\"value_str\", col(\"value\").cast(\"string\"))\n",
    "df_data = df_parsed.withColumn(\"data\", from_json(col(\"value_str\"), schema))\n",
    "\n",
    "# Memilih kolom yang diinginkan\n",
    "df_result = df_data.select(\n",
    "    \"data.*\",  # Semua kolom dari data yang diparsing\n",
    "    \"key\", \"topic\", \"partition\", \"offset\", \"timestamp\", \"timestampType\"  # Metadata Kafka\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacffab",
   "metadata": {},
   "source": [
    "**5️⃣ Menampilkan Data Secara Streaming**\n",
    "\n",
    "Sekarang, kita akan menampilkan hasil streaming. Karena ini adalah operasi **streaming**, data akan terus diproses dan ditampilkan secara real-time.\n",
    "\n",
    "Beberapa parameter yang bisa digunakan untuk streaming data:\n",
    "\n",
    "- **`outputMode(\"append\")`**: Mode ini menambahkan baris baru ke output tanpa mengubah data yang sudah ada.\n",
    "- **`format(\"console\")`**: Menulis output stream ke konsol. Alternatif lain adalah `\"memory\"` untuk menyimpan data di memori.\n",
    "- **`awaitTermination()`**: Menjaga query tetap berjalan tanpa batas waktu, menunggu data baru untuk diproses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a3939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tulis hasil stream ke konsol dalam mode append\n",
    "query = (\n",
    "    df_result\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")  # Menambahkan data baru ke output\n",
    "    .format(\"console\")  # Menampilkan hasil di konsol (bisa juga 'memory' untuk query di memori)\n",
    "    .start()  # Memulai streaming query\n",
    ")\n",
    "\n",
    "# Menunggu stream berjalan\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044dbc50",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert\">\n",
    "<p>\n",
    "\n",
    "Pada langkah ini, kita telah membuat **Kafka consumer** yang membaca data secara terus-menerus dari topik Kafka. Dengan **PySpark**, kita berhasil:\n",
    "1. Membaca data streaming dari Kafka.\n",
    "2. Memparsing pesan JSON dari format binary menjadi terstruktur.\n",
    "3. Menampilkan hasil data secara real-time di konsol.\n",
    "</p>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26cd22",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\"><br>\n",
    "  <center><h2>Streaming Data From Kafka + PySpark for Prediction</h2></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e85a94",
   "metadata": {},
   "source": [
    "## Streaming Data dari Kafka + PySpark untuk Prediksi\n",
    "\n",
    "Pada bagian ini kita akan menerapkan **model machine learning** yang telah dilatih sebelumnya dalam skenario streaming, dengan memanfaatkan **Kafka** untuk menerima data secara real-time, **PySpark** untuk pemrosesan data, dan **model prediksi** untuk memberikan hasil secara langsung. Berikut langkah-langkahnya:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38424a0c",
   "metadata": {},
   "source": [
    "**1️⃣ Membuat Spark Session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Membuat Spark session untuk menghubungkan ke cluster Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Credit Data Kafka Predict\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,org.apache.kafka:kafka-clients:3.8.1\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4)\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f00abb",
   "metadata": {},
   "source": [
    "**2️⃣ Mendefinisikan Skema untuk Data Masuk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16cccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "# Define schema for the data in Kafka messages\n",
    "schema = StructType([\n",
    "    StructField(\"person_home_ownership\", StringType(), True),\n",
    "    StructField(\"loan_intent\", StringType(), True),\n",
    "    StructField(\"loan_grade\", StringType(), True),\n",
    "    StructField(\"person_age\", IntegerType(), True),\n",
    "    StructField(\"person_income\", FloatType(), True),\n",
    "    StructField(\"person_emp_length\", IntegerType(), True),\n",
    "    StructField(\"loan_amnt\", FloatType(), True),\n",
    "    StructField(\"loan_int_rate\", FloatType(), True),\n",
    "    StructField(\"cb_person_cred_hist_length\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9333fe",
   "metadata": {},
   "source": [
    "**3️⃣ Membaca Data Streaming dari Kafka**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa757334",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_kafka_df_predict = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"loan_applications\")\n",
    "    .option(\"startingOffsets\", \"earliest\")  # Start reading from the earliest message\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1047fcbe",
   "metadata": {},
   "source": [
    "**4️⃣ Memuat Model yang Telah Dilatih**\n",
    "\n",
    "Kita akan memuat **model pre-trained** yang sebelumnya telah dilatih menggunakan data historis. Model ini akan digunakan untuk memprediksi hasil aplikasi pinjaman baru yang diterima melalui Kafka.\n",
    "\n",
    "Model yang dimuat:\n",
    "\n",
    "- **RandomForestClassificationModel**: Model untuk memprediksi apakah aplikasi pinjaman akan disetujui atau ditolak.\n",
    "- **MinMaxScalerModel**: Model untuk **scaling** fitur ke dalam rentang standar.\n",
    "- **PipelineModel**: Mengelola preprocessing seperti encoding, pengisian nilai yang hilang, dan transformasi fitur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a663a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.ml.feature import MinMaxScalerModel, VectorAssembler\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Load pre-trained models\n",
    "rf_model = RandomForestClassificationModel.load(\"models_new1/random_forest_credit_risk\")\n",
    "scaler_model = MinMaxScalerModel.load(\"models_new1/minmax_scaler\")\n",
    "pipeline_model = PipelineModel.load(\"models_new1/preprocessing_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5c2d2",
   "metadata": {},
   "source": [
    "**5️⃣ Parsing dan Preprocessing Data**\n",
    "\n",
    "Pesan dari Kafka berbentuk **binary**, sehingga kita perlu mengubahnya ke string, parsing JSON, lalu menyesuaikan dengan skema yang sudah dibuat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a601fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "\n",
    "# Parsing JSON dari kolom 'value' Kafka\n",
    "df_parsed = stream_kafka_df_predict.withColumn(\"value_str\", col(\"value\").cast(\"string\"))\n",
    "df_data = df_parsed.withColumn(\"data\", from_json(col(\"value_str\"), schema)).select(\"data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f7b9a",
   "metadata": {},
   "source": [
    "**6️⃣ Definisi Logika Prediksi dan Penerapan Pipeline**\n",
    "\n",
    "Langkah ini mendefinisikan logika untuk memproses data streaming dan menggunakan model yang sudah dilatih untuk prediksi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define function to apply the prediction and add the result column\n",
    "def predict_and_add_column(input_data):\n",
    "    # Apply preprocessing pipeline (e.g., encoding categorical variables)\n",
    "    processed_data = pipeline_model.transform(input_data)\n",
    "    \n",
    "    # Assemble features into a single vector column\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=['person_age', 'person_income', 'person_emp_length', 'loan_amnt', 'loan_int_rate',\n",
    "                   'cb_person_cred_hist_length', 'person_home_ownership_encoded', 'loan_intent_encoded', 'loan_grade_encoded'],\n",
    "        outputCol=\"features_unscaled\"\n",
    "    )\n",
    "    assembled_data = assembler.transform(processed_data)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaled_data = scaler_model.transform(assembled_data)\n",
    "    \n",
    "    # Predict the loan approval (1 = Approved, 0 = Denied)\n",
    "    predictions = rf_model.transform(scaled_data)\n",
    "    \n",
    "    # Add 'approval_status' column based on prediction\n",
    "    result_data = predictions.withColumn(\n",
    "        \"approval_status\",\n",
    "        F.when(col(\"prediction\") == 1, \"Approved\").otherwise(\"Denied\")\n",
    "    )\n",
    "    \n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a14d16",
   "metadata": {},
   "source": [
    "<div class=\"alert\">\n",
    "  <details>\n",
    "    <summary><b>Penjelasan</b></summary> \n",
    "    \n",
    "**1. Preprocessing Data**\n",
    "Pertama-tama, kita menggunakan **pipeline model** yang sudah dilatih untuk memproses data yang masuk. Pipeline ini memastikan bahwa fitur kategori di-encode dengan benar dan data ditransformasikan ke dalam format yang dibutuhkan oleh model machine learning.\n",
    "\n",
    "- **Preprocessing** di pipeline model dapat mencakup berbagai langkah, seperti mengisi nilai yang hilang, encoding variabel kategorikal (misalnya, `person_home_ownership`), dan mengubah format data agar sesuai untuk input model.\n",
    "\n",
    "**2. Menyusun Fitur**\n",
    "Selanjutnya, kita menyusun semua fitur individu menjadi satu vektor fitur yang tunggal. Proses ini dilakukan dengan menggunakan **VectorAssembler**, yang menggabungkan kolom-kolom fitur menjadi satu kolom vektor. Ini adalah langkah penting karena banyak model machine learning (termasuk Random Forest) membutuhkan input dalam bentuk vektor fitur.\n",
    "\n",
    "**3. Scaling Fitur**\n",
    "Fitur-fitur yang telah disusun dalam vektor kemudian akan **diskalakan** menggunakan **MinMaxScaler**. Scaling bertujuan untuk menormalkan nilai-nilai fitur sehingga berada dalam rentang yang sama. Hal ini penting karena banyak model machine learning, termasuk Random Forest, lebih efektif bila fitur-fitur memiliki skala yang seragam.\n",
    "\n",
    "**4. Prediksi dengan Model Random Forest**\n",
    "Setelah fitur-fitur diskalakan, kita mengaplikasikan model **RandomForest** yang telah dilatih sebelumnya untuk melakukan prediksi status persetujuan pinjaman. Model ini akan menghasilkan prediksi dalam bentuk nilai numerik, yaitu `1` untuk disetujui (approved) dan `0` untuk ditolak (denied).\n",
    "\n",
    "**5. Menambahkan Kolom Prediksi**\n",
    "Setelah prediksi dihasilkan, kita menambahkan kolom baru, yaitu **approval_status**, yang berisi status persetujuan pinjaman berdasarkan hasil prediksi. Jika prediksi bernilai `1`, maka kolom `approval_status` akan berisi \"Approved\", dan jika bernilai `0`, kolom tersebut akan berisi \"Denied\".\n",
    "\n",
    "    \n",
    "</details>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f4267",
   "metadata": {},
   "source": [
    "**7️⃣ Streaming Prediksi dan Menampilkan Hasil**\n",
    "\n",
    "Setelah fungsi prediksi diterapkan pada data streaming, hasil prediksi akan ditampilkan secara real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a6c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply prediction logic on the streaming data\n",
    "predicted_stream = predict_and_add_column(df_data)\n",
    "\n",
    "# Output the results to the console (for debugging or display in Jupyter notebook)\n",
    "query = (\n",
    "    predicted_stream\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")  # Keep adding new predictions as data comes in\n",
    "    .format(\"console\")  # Display results in console\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Wait for the stream to finish\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ba9900",
   "metadata": {},
   "source": [
    "<div class=\"alert\">\n",
    "<p>\n",
    "\n",
    "Langkah ini mengintegrasikan Kafka, PySpark, dan model machine learning untuk memproses dan memprediksi data real-time. **Manfaatnya**:\n",
    "1. **Real-time Processing**: Prediksi dilakukan segera setelah data diterima.\n",
    "2. **Scalable Architecture**: Spark dan Kafka memungkinkan pengolahan data dalam skala besar.\n",
    "3. **Efisiensi Model**: Memanfaatkan pipeline dan model pre-trained untuk alur kerja yang konsisten.\n",
    "\n",
    "</p>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4410a3",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-warning\"><br>\n",
    "  <center><h1>Additional: Other Big Data Tools</h1></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c4b91f",
   "metadata": {},
   "source": [
    "# Additional: Other Big Data Tools\n",
    "\n",
    "Pada bagian sebelumnya, kita menggunakan **Kafka** untuk melakukan streaming data keuangan secara real-time dan **Spark** untuk memprosesnya. Bayangkan Kafka seperti *layanan pengirim pesan* yang mengirimkan data aplikasi pinjaman (atau data keuangan lainnya) dari satu tempat ke tempat lain secara real-time. Spark kemudian menganalisis data ini untuk memprediksi hasil, seperti apakah pinjaman harus disetujui atau ditolak.\n",
    "\n",
    "Namun, Kafka dan Spark hanyalah dua alat dari banyak alat yang tersedia dalam ekosistem **Big Data**. Mari kita jelajahi beberapa alat lainnya yang sangat berguna dalam dunia keuangan untuk menangani dataset yang sangat besar, yang sering disebut **Big Data**.\n",
    "\n",
    "Alat-alat ini melengkapi PySpark dalam pemrosesan data keuangan, memungkinkan penanganan data dari log transaksi hingga wawasan pelanggan secara efisien."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e47d5",
   "metadata": {},
   "source": [
    "## **1. Hadoop**\n",
    "\n",
    "- **Apa itu Hadoop?**\n",
    "  - Apache Hadoop adalah kerangka kerja sumber terbuka yang memungkinkan penyimpanan dan pemrosesan data besar secara terdistribusi di seluruh kluster komputer. Hadoop dirancang untuk menangani big data dengan membagi data menjadi bagian-bagian kecil dan memprosesnya secara paralel.\n",
    "  - Hadoop memiliki dua komponen utama:\n",
    "    1. **Hadoop Distributed File System (HDFS):** Sistem penyimpanan yang dapat diskalakan dan tahan terhadap kegagalan, yang mendistribusikan data ke beberapa mesin sambil menjaga integritas data melalui replikasi.\n",
    "    2. **MapReduce (atau YARN untuk versi yang lebih baru):** Mesin pemrosesan yang memungkinkan komputasi pada dataset yang terdistribusi.\n",
    "\n",
    "- **Mengapa menggunakan Hadoop?**\n",
    "  \n",
    "  1. **Skalabilitas:** Hadoop dapat menangani data dalam jumlah sangat besar, bahkan hingga petabytes, dengan menambah mesin baru ke dalam kluster (skala horizontal).\n",
    "  2. **Toleransi Terhadap Kegagalan:** HDFS secara otomatis mereplikasi data di beberapa node, memastikan data tetap tersedia meskipun ada node yang gagal.\n",
    "  3. **Efisiensi Biaya:** Hadoop menggunakan perangkat keras komoditas, sehingga lebih terjangkau dibandingkan solusi penyimpanan tradisional.\n",
    "  4. **Adopsi dan Integrasi yang Luas:** Hadoop terintegrasi dengan baik dengan alat-alat lain dalam ekosistem big data, seperti Apache Spark.\n",
    "\n",
    "- **Bagaimana Hadoop bekerja dengan PySpark?**: PySpark dapat langsung membaca dan memproses data yang disimpan di HDFS milik Hadoop. Misalnya, jika Anda telah menyimpan data pinjaman masa lalu di Hadoop, Anda dapat menggunakan PySpark untuk memuatnya dan membangun model seperti yang kita bahas sebelumnya.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1b48c1",
   "metadata": {},
   "source": [
    "\n",
    "### **Skenario:**\n",
    "\n",
    "Sebuah bank ingin memproses aplikasi pinjaman secara real-time menggunakan Kafka dan PySpark, memprediksi persetujuan atau penolakan menggunakan model machine learning. Selain pemrosesan real-time, bank tersebut **menyimpan hasil yang sudah diproses dan data mentah di Hadoop untuk penyimpanan jangka panjang, pelatihan ulang model, dan kepatuhan terhadap regulasi.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Penjelasan: Menambahkan Hadoop ke Alur Kerja**\n",
    "\n",
    "#### **Mengapa Menambahkan Hadoop?**\n",
    "- **Penyimpanan Data Historis:** Menyimpan aplikasi pinjaman masa lalu dan keputusan untuk keperluan audit dan analisis.\n",
    "- **Pemrosesan Batch:** Menggunakan data historis untuk melatih ulang model machine learning secara berkala.\n",
    "- **Backup dan Arsip:** Memastikan tidak ada data yang hilang jika masa retensi topik Kafka terlewat.\n",
    "\n",
    "---\n",
    "\n",
    "### **Langkah-langkah Implementasi dengan Hadoop**\n",
    "\n",
    "#### **1. Menyimpan Data Mentah dari Kafka ke Hadoop (HDFS)**\n",
    "\n",
    "Setelah mem-parsing data dari Kafka, kita simpan data aplikasi pinjaman mentah ke HDFS. Ini memastikan bahwa semua data yang masuk disimpan untuk penggunaan di masa depan.\n",
    "\n",
    "```python\n",
    "# Simpan data mentah Kafka ke HDFS\n",
    "raw_data_hdfs_path = \"hdfs://localhost:9000/loan_monitoring/raw_data\"\n",
    "df_data.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", raw_data_hdfs_path) \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:9000/loan_monitoring/checkpoints/raw_data\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Menyimpan Hasil yang Diproses ke Hadoop**\n",
    "\n",
    "Setelah prediksi dilakukan, hasilnya disimpan ke HDFS untuk penyimpanan jangka panjang.\n",
    "\n",
    "```python\n",
    "# Simpan hasil prediksi yang telah diproses ke HDFS\n",
    "processed_results_hdfs_path = \"hdfs://localhost:9000/loan_monitoring/processed_results\"\n",
    "predicted_stream.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", processed_results_hdfs_path) \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:9000/loan_monitoring/checkpoints/processed_results\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Menggunakan Data HDFS untuk Pelatihan Ulang Model**\n",
    "\n",
    "Data aplikasi pinjaman historis yang disimpan di HDFS dapat diambil dan digunakan untuk melatih ulang model machine learning secara berkala.\n",
    "\n",
    "```python\n",
    "# Muat data historis dari HDFS\n",
    "historical_data = spark.read.format(\"parquet\").load(\"hdfs://localhost:9000/loan_monitoring/processed_results\")\n",
    "\n",
    "# Lakukan pelatihan ulang model\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[...], outputCol=\"features\")\n",
    "training_data = assembler.transform(historical_data).select(\"features\", \"label\")\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\n",
    "rf_model_updated = rf.fit(training_data)\n",
    "\n",
    "# Simpan model yang telah diperbarui\n",
    "rf_model_updated.save(\"models_new1/random_forest_credit_risk_updated\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Kesimpulan**\n",
    "Dengan menambahkan Hadoop ke dalam alur kerja, kita dapat menangani data dalam skala besar secara efisien. Hadoop menyediakan solusi untuk menyimpan data secara terdistribusi dan memastikan data tetap tersedia bahkan jika terjadi kegagalan. Ini juga memungkinkan kita untuk melakukan pemrosesan batch dan pelatihan ulang model menggunakan data historis yang disimpan di HDFS. Sehingga, kombinasi Kafka, Spark, dan Hadoop membantu bank atau lembaga keuangan dalam mengelola dan menganalisis data besar secara efektif."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc9a01",
   "metadata": {},
   "source": [
    "\n",
    "## **2. Hive**\n",
    "\n",
    "- **What it is**: Hive is a data warehouse tool built on top of Hadoop. It allows you to query large datasets stored in Hadoop using SQL-like commands.\n",
    "- **Why Use Hive?**\n",
    "  1. **SQL-Like Querying on Big Data**: Hive allows financial analysts to use SQL queries to explore and analyze historical loan data without requiring advanced programming skills.  \n",
    "  2. **Integration with Hadoop**: Hive operates directly on data stored in HDFS, making it ideal for querying raw and processed data from the loan monitoring pipeline.  \n",
    "  3. **Schema and Table Management**: It provides a structured view of data with predefined schemas, improving organization and ease of use.  \n",
    "  4. **Batch Analytics**: Hive can be used for complex aggregations and batch analytics on stored loan data.  \n",
    "- **How it works with PySpark**: PySpark can query data stored in Hive using SQL. For example, you could pull all records of customers who applied for loans in the past year and process that data for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ce918",
   "metadata": {},
   "source": [
    "### **Scenario: Integrating Hive into Real-Time Loan Monitoring**\n",
    "\n",
    "Apache Hive can enhance the **Real-Time Loan Monitoring** workflow by enabling SQL-based querying on top of Hadoop (HDFS). Hive is particularly useful for financial institutions as it simplifies querying large datasets stored in Hadoop and integrates well with PySpark.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps Breakdown for Hive Integration**\n",
    "\n",
    "#### 1. **Configure Hive Metastore with PySpark**\n",
    "PySpark can seamlessly connect to Hive using its **HiveContext**. To enable this, the Hive metastore must be configured correctly.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Credit Data Hive Integration\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://localhost:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark\n",
    "```\n",
    "- **`spark.sql.catalogImplementation`**: Configures Hive as the catalog for SQL operations.  \n",
    "- **`hive.metastore.uris`**: Specifies the Hive metastore's address (ensure Hive is running).  \n",
    "- **`enableHiveSupport()`**: Enables Hive table integration within the Spark session.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Create Hive Tables for Loan Data**\n",
    "\n",
    "Hive tables are created to organize and query data.  \n",
    "- **Raw Data Table:** Stores raw Kafka messages for archival and compliance.  \n",
    "- **Processed Data Table:** Stores predictions (e.g., loan approval/denial decisions).  \n",
    "\n",
    "```sql\n",
    "-- Raw loan application data table\n",
    "CREATE TABLE IF NOT EXISTS raw_loan_data (\n",
    "    person_home_ownership STRING,\n",
    "    loan_intent STRING,\n",
    "    loan_grade STRING,\n",
    "    person_age INT,\n",
    "    person_income FLOAT,\n",
    "    person_emp_length INT,\n",
    "    loan_amnt FLOAT,\n",
    "    loan_int_rate FLOAT,\n",
    "    cb_person_cred_hist_length INT\n",
    ")\n",
    "STORED AS PARQUET;\n",
    "\n",
    "-- Processed predictions table\n",
    "CREATE TABLE IF NOT EXISTS processed_loan_predictions (\n",
    "    person_id STRING,\n",
    "    prediction INT,\n",
    "    approval_status STRING,\n",
    "    timestamp TIMESTAMP\n",
    ")\n",
    "STORED AS PARQUET;\n",
    "```\n",
    "- **`STORED AS PARQUET`**: Stores data in an efficient, columnar format for faster queries.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Save Data from PySpark to Hive**\n",
    "\n",
    "Once data is processed using PySpark, save it directly into the respective Hive tables.  \n",
    "\n",
    "```python\n",
    "# Write raw data to Hive\n",
    "df_data.write \\\n",
    "    .format(\"hive\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"raw_loan_data\")\n",
    "\n",
    "# Write processed predictions to Hive\n",
    "predicted_stream.writeStream \\\n",
    "    .format(\"hive\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/processed_loan_predictions\") \\\n",
    "    .toTable(\"processed_loan_predictions\")\n",
    "```\n",
    "- **`saveAsTable()`**: Writes batch data to a Hive table.  \n",
    "- **`toTable()`**: Writes streaming data directly to Hive tables.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Query Hive Tables for Analysis**\n",
    "\n",
    "Hive’s SQL interface makes it easy to query and analyze stored data, enabling financial analysts to extract insights.  \n",
    "\n",
    "```python\n",
    "# Query raw loan data\n",
    "raw_query = spark.sql(\"\"\"\n",
    "    SELECT loan_intent, COUNT(*) AS application_count\n",
    "    FROM raw_loan_data\n",
    "    GROUP BY loan_intent\n",
    "    ORDER BY application_count DESC\n",
    "\"\"\")\n",
    "raw_query.show()\n",
    "\n",
    "# Query approved loans from processed data\n",
    "processed_query = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) AS approved_loans\n",
    "    FROM processed_loan_predictions\n",
    "    WHERE approval_status = 'Approved'\n",
    "\"\"\")\n",
    "processed_query.show()\n",
    "```\n",
    "- SQL queries run directly on the Hive tables stored in HDFS.  \n",
    "- Analysts can perform aggregations, filter data, and join tables using familiar SQL syntax.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Use Case: Model Retraining with Hive**\n",
    "Hive simplifies model retraining by providing a structured view of historical data, which can be used to feed new training datasets.\n",
    "\n",
    "```python\n",
    "# Load training data from Hive\n",
    "training_data = spark.sql(\"\"\"\n",
    "    SELECT person_age, person_income, person_emp_length, loan_amnt, loan_int_rate,\n",
    "           cb_person_cred_hist_length, prediction AS label\n",
    "    FROM processed_loan_predictions\n",
    "\"\"\")\n",
    "\n",
    "# Train a new Random Forest model\n",
    "assembler = VectorAssembler(inputCols=[\"person_age\", \"person_income\", ...], outputCol=\"features\")\n",
    "training_data = assembler.transform(training_data).select(\"features\", \"label\")\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "new_rf_model = rf.fit(training_data)\n",
    "\n",
    "# Save updated model\n",
    "new_rf_model.save(\"models_new1/random_forest_credit_risk_updated\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Summary of Hive-Enhanced Workflow**\n",
    "\n",
    "1. **Hive as Data Store:** Hive organizes and stores both raw and processed loan application data.  \n",
    "2. **SQL Querying:** Simplifies data access for non-technical users (e.g., financial analysts).  \n",
    "3. **Streamlined Model Updates:** Hive tables provide clean, structured datasets for retraining models.  \n",
    "4. **Regulatory Compliance:** Hive enables quick retrieval of historical data for audits.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8533196e",
   "metadata": {},
   "source": [
    "## **3. Impala**\n",
    "- **What it is**: Impala is another SQL-based query tool that works with data stored in Hadoop. Unlike Hive, it is optimized for speed, making it a great choice for **real-time analytics**.\n",
    "- **Why Use Impala?**\n",
    "\n",
    "  1. **Real-Time Analytics**: Impala provides near real-time querying on data stored in HDFS or Apache Kudu.  \n",
    "  2. **High Performance**: Its in-memory execution engine outperforms Hive for low-latency queries.  \n",
    "  3. **Interactive Analytics**: Financial analysts can run complex queries interactively without waiting for batch jobs.  \n",
    "  4. **Integration with BI Tools**: Impala integrates well with tools like Tableau and Power BI for dashboard creation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef5fa0",
   "metadata": {},
   "source": [
    "### **Scenario: Integrating Impala into Real-Time Loan Monitoring**\n",
    "\n",
    "**Apache Impala** adds high-performance, SQL-based querying on top of Hadoop (HDFS). Compared to Hive, Impala is designed for **low-latency queries**, making it ideal for interactive analytics and scenarios where query response times are critical.\n",
    "\n",
    "\n",
    "\n",
    "### **Steps Breakdown for Impala Integration**\n",
    "\n",
    "#### 1. **Configure Impala for PySpark**\n",
    "PySpark can connect to Impala using **JDBC/ODBC** connectors or by querying Impala tables via **HiveContext**.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with Hive and Impala support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Credit Data Impala Integration\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://localhost:9083\") \\\n",
    "    .config(\"spark.sql.hive.hiveserver2.jdbc.url\", \"jdbc:hive2://localhost:10000/default\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark\n",
    "```\n",
    "\n",
    "- **Impala Hive Integration**: Impala queries Hive’s metastore for table metadata, so the same `enableHiveSupport` setup is used.  \n",
    "- **HiveServer2 JDBC URL**: Configured to enable JDBC connectivity for PySpark.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Create Impala Tables for Loan Data**\n",
    "Impala uses Hive’s metastore for table definitions. Tables created for Hive are automatically accessible in Impala.\n",
    "\n",
    "```sql\n",
    "-- Raw loan application data table\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS raw_loan_data (\n",
    "    person_home_ownership STRING,\n",
    "    loan_intent STRING,\n",
    "    loan_grade STRING,\n",
    "    person_age INT,\n",
    "    person_income FLOAT,\n",
    "    person_emp_length INT,\n",
    "    loan_amnt FLOAT,\n",
    "    loan_int_rate FLOAT,\n",
    "    cb_person_cred_hist_length INT\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION 'hdfs://localhost:9000/loan_monitoring/raw_data';\n",
    "\n",
    "-- Processed predictions table\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS processed_loan_predictions (\n",
    "    person_id STRING,\n",
    "    prediction INT,\n",
    "    approval_status STRING,\n",
    "    timestamp TIMESTAMP\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION 'hdfs://localhost:9000/loan_monitoring/processed_predictions';\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Save Data from PySpark to HDFS**\n",
    "Data saved to HDFS is automatically available to Impala tables because of the **external table** definition.\n",
    "\n",
    "```python\n",
    "# Write raw data to HDFS for Impala\n",
    "df_data.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"hdfs://localhost:9000/loan_monitoring/raw_data\")\n",
    "\n",
    "# Write processed predictions to HDFS for Impala\n",
    "predicted_stream.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"hdfs://localhost:9000/loan_monitoring/processed_predictions\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/processed_predictions\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Query Impala Tables for Real-Time Analytics**\n",
    "\n",
    "Impala enables fast querying of stored data using SQL. Queries can be executed interactively or from PySpark.\n",
    "\n",
    "```python\n",
    "# Query raw loan data using Spark SQL (via Impala tables)\n",
    "raw_query = spark.sql(\"\"\"\n",
    "    SELECT loan_intent, COUNT(*) AS application_count\n",
    "    FROM raw_loan_data\n",
    "    GROUP BY loan_intent\n",
    "    ORDER BY application_count DESC\n",
    "\"\"\")\n",
    "raw_query.show()\n",
    "\n",
    "# Query approved loans from processed predictions\n",
    "approved_query = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) AS approved_loans\n",
    "    FROM processed_loan_predictions\n",
    "    WHERE approval_status = 'Approved'\n",
    "\"\"\")\n",
    "approved_query.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Interactive Dashboards with Impala**\n",
    "Impala’s low-latency querying makes it ideal for connecting to BI tools like Tableau or Power BI.  \n",
    "\n",
    "- **Setup**: Connect Tableau/Power BI to Impala using JDBC/ODBC.  \n",
    "- **Visualize**: Create dashboards for KPIs like loan approval trends or income demographics.  \n",
    "- Example Dashboard Metrics:  \n",
    "  - **Approval Rate by Loan Intent**: Percentage of loans approved for different intents.  \n",
    "  - **Average Loan Amount by Age Group**: Trends in loan sizes across age groups.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Use Case: Model Retraining with Impala**\n",
    "Impala tables can provide the training dataset for retraining machine learning models. PySpark retrieves data from Impala for preprocessing.\n",
    "\n",
    "```python\n",
    "# Query data for retraining\n",
    "training_data_query = \"\"\"\n",
    "    SELECT person_age, person_income, person_emp_length, loan_amnt, loan_int_rate,\n",
    "           cb_person_cred_hist_length, prediction AS label\n",
    "    FROM processed_loan_predictions\n",
    "\"\"\"\n",
    "training_data = spark.sql(training_data_query)\n",
    "\n",
    "# Prepare training data\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"person_age\", \"person_income\", ...], outputCol=\"features\")\n",
    "training_data = assembler.transform(training_data).select(\"features\", \"label\")\n",
    "\n",
    "# Train updated model\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "new_rf_model = rf.fit(training_data)\n",
    "\n",
    "# Save updated model\n",
    "new_rf_model.save(\"models_new1/random_forest_credit_risk_updated\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Impala-Enhanced Workflow**\n",
    "\n",
    "1. **Impala for Interactive Queries**: Enables low-latency SQL queries on data stored in HDFS.  \n",
    "2. **External Tables for Integration**: Impala seamlessly accesses data written by PySpark to HDFS.  \n",
    "3. **Real-Time Insights**: Impala supports real-time analytics for financial metrics like approval rates.  \n",
    "4. **Seamless BI Tool Integration**: Connect Impala to Tableau/Power BI for interactive dashboards.  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5621d64",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this module, we explored how to integrate **real-time data processing** with **Big Data tools** for financial applications. Here's what we covered:\n",
    "\n",
    "1. **Streaming Data with Kafka**  \n",
    "   - We used Kafka as a **message broker** to handle real-time loan application data, showcasing how streaming data can be ingested and processed continuously.\n",
    "\n",
    "2. **Real-Time Predictions with PySpark**  \n",
    "   - PySpark was used to process the streamed data and apply machine learning models for predicting **loan approval statuses** in real time.  \n",
    "   - We demonstrated how to load pre-trained models, preprocess incoming data, and generate predictions, all within a streaming pipeline.\n",
    "\n",
    "3. **Complementary Big Data Tools**  \n",
    "   - **Hadoop**: For storing large-scale financial data like transaction logs or historical loan applications.  \n",
    "   - **Hive**: For querying big datasets using SQL, making it accessible to finance professionals familiar with traditional databases.  \n",
    "   - **Impala**: For fast, real-time querying and reporting, ideal for dashboards or fraud detection.\n",
    "\n",
    "## Key Takeaway:\n",
    "\n",
    "This module demonstrated the power of combining tools like Kafka, PySpark, and Big Data frameworks to build efficient, real-time data pipelines in the finance sector. By mastering these technologies, financial institutions can better handle the challenges of large datasets, provide instant insights, and enhance decision-making processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kafka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
