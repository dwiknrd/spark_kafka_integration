{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"padding: 60px;\n",
    "  text-align: center;\n",
    "  background: #d4afb9;\n",
    "  color: #003049;\n",
    "  font-size: 20px;\">\n",
    "  <h2>Inclass: Spark Integration for Streaming Data</h2>\n",
    "   <hr>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f986ae13",
   "metadata": {},
   "source": [
    "# 🌟 Introduction to Data Streaming  \n",
    "\n",
    "📈 **Real-time data** adalah kunci di dunia keuangan yang bergerak cepat. **Data streaming** memungkinkan lembaga keuangan menganalisis data besar seperti harga saham, tren pasar, atau transaksi secara langsung. Dengan ini, keputusan dapat diambil cepat, risiko dimonitor, dan kecurangan dideteksi.  \n",
    "\n",
    "🎯 **Contoh Penggunaan Data Streaming**:  \n",
    "\n",
    "- 💹 **Analisis Pasar Saham**: Memantau harga saham dan membuat keputusan trading secara real-time.  \n",
    "- 🚨 **Deteksi Kecurangan**: Identifikasi aktivitas mencurigakan untuk mengurangi risiko.  \n",
    "- 🛒 **Pantauan Transaksi Pelanggan**: Tawarkan layanan personal berdasarkan pola transaksi langsung.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b9141",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🚀 Spark Structured Streaming  \n",
    "\n",
    "**Spark Structured Streaming** adalah ekstensi dari Apache Spark yang memproses data **real-time** menggunakan API sederhana yang sama dengan batch processing.  \n",
    "\n",
    "✨ **Kenapa Pilih Spark Structured Streaming?**  \n",
    "- 🧑‍💻 **Skalabilitas**: Mampu menangani data dalam jumlah besar.  \n",
    "- 🛡️ **Toleransi Kesalahan**: Data aman dengan fitur checkpointing.  \n",
    "- ⏱️ **Analisis Real-Time**: Query data langsung dengan sintaks SQL-like.  \n",
    "- 🔄 **Kode Terpadu**: Gunakan kode yang sama untuk batch dan streaming.  \n",
    "\n",
    "💡 Spark Structured Streaming memproses data dalam **micro-batches**, yaitu membagi aliran data menjadi potongan kecil agar lebih mudah dikelola.  \n",
    "\n",
    "📘 Lihat panduan lengkapnya di [Spark Documentation](https://spark.apache.org/docs/3.5.1/structured-streaming-programming-guide.html).  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66cf7b9",
   "metadata": {},
   "source": [
    "## 🧠 Basic Concept  \n",
    "\n",
    "### 🔄 Stream Processing  \n",
    "\n",
    "🏦 Di industri keuangan, stream processing membantu menganalisis data sensitif waktu seperti pergerakan harga saham atau log transaksi pelanggan. Dengan **Structured Streaming**, respons cepat terhadap perubahan pasar atau deteksi fraud menjadi mungkin.  \n",
    "\n",
    "💡 Bayangkan data stream sebagai **“Input Table”**: Setiap data baru adalah baris baru dalam tabel. Bedanya, pada batch processing, semua data diproses sekaligus, sedangkan pada streaming, data diproses terus-menerus saat datang.  \n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Cara Kerja:  \n",
    "\n",
    "1. **Data sebagai Tabel**:  \n",
    "   Bayangkan data saham masuk setiap detik. Setiap harga baru jadi baris baru di tabel yang dapat langsung di-query.  \n",
    "\n",
    "2. **Micro-Batching**:  \n",
    "   Alih-alih memproses satu per satu, data dikumpulkan dalam batch kecil untuk efisiensi.  \n",
    "\n",
    "3. **Proses Real-Time**:  \n",
    "   Data langsung dianalisis, diubah, atau dihitung layaknya batch processing.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧾 Contoh Real-Time Table  \n",
    "\n",
    "![](assets/structured-streaming-model.png)\n",
    "\n",
    "1. **Input Table**:  \n",
    "   Kosong di awal, lalu tumbuh saat data baru masuk (contoh: harga saham).  \n",
    "\n",
    "2. **Proses**:  \n",
    "   - 📊 Hitung rata-rata pergerakan harga saham.  \n",
    "   - 📈 Analisis tren perubahan harga mendadak.  \n",
    "   - 🕒 Agregasi data dalam rentang waktu (contoh: rata-rata harga 5 menit terakhir).  \n",
    "\n",
    "3. **Result Table**:  \n",
    "   Hasil di-update terus ke tabel, layar, atau database untuk mendukung **pengambilan keputusan real-time**.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa61547",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><br>\n",
    "  <center><h2>🔄 Workflow Spark Structured Streaming  </h2></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e991d51",
   "metadata": {},
   "source": [
    "# 🔄 Workflow Spark Structured Streaming  \n",
    "\n",
    "## 💻 Contoh: Pantauan Transaksi Real-Time  \n",
    "\n",
    "💡 Bayangkan data transaksi pelanggan dalam format **JSON**. Kita bisa:  \n",
    "- Menghitung total transaksi per pelanggan.  \n",
    "- Mendeteksi pola perilaku secara langsung.  \n",
    "\n",
    "###  1️⃣ **Impor Library dan Buat `SparkSession`**  \n",
    "\n",
    "Untuk memulai:  \n",
    "1. 🛠️ Impor library:  \n",
    "   - `StructType` & `StructField`: Mendefinisikan skema JSON.  \n",
    "   - `from_json` & `col`: Parsing dan interaksi dengan data JSON.  \n",
    "   - `sum`: Untuk operasi agregasi.  \n",
    "2. 🚀 Buat `SparkSession`: Pusat operasi Spark, termasuk untuk Structured Streaming.  \n",
    "\n",
    "✨ **SparkSession** adalah langkah awal untuk memproses aliran data dengan Spark.  \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0dc86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import from_json, col, sum\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TransactionStreaming\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15ceaf",
   "metadata": {},
   "source": [
    "### 2️⃣ **Tentukan Schema Data**  \n",
    "\n",
    "Schema mendefinisikan struktur data yang akan diterima. Gunakan `StructType` dan `StructField`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764311d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the transaction data\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"transaction_amount\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6b438d",
   "metadata": {},
   "source": [
    "📋 **Penjelasan**:  \n",
    "   - **`customer_id`**: ID unik pelanggan (String).  \n",
    "   - **`transaction_amount`**: Jumlah transaksi (Double).  \n",
    "   - **Nullable (`True`)**: Kolom dapat berisi nilai kosong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92dfec3",
   "metadata": {},
   "source": [
    "### **3️⃣ Baca Data Streaming dengan Spark**\n",
    "\n",
    "Kita menggunakan **`readStream`** untuk membaca data dari sumber streaming. Dalam contoh ini, data real-time disimulasikan melalui **socket**.\n",
    "\n",
    "**🔌 Sumber Data: Socket**\n",
    "\n",
    "Socket digunakan untuk mengirim data real-time.  \n",
    "Konfigurasi yang diperlukan:  \n",
    "\n",
    "- **`host`**: Lokasi server data (contoh: `localhost`).  \n",
    "- **`port`**: Nomor port untuk koneksi (contoh: `9999`).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afba755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the stream of transaction data from a socket\n",
    "raw_stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b363cb",
   "metadata": {},
   "source": [
    "**📦 Parsing Data JSON**\n",
    "\n",
    "Data mentah dalam format JSON diubah menjadi kolom terstruktur dengan langkah berikut:  \n",
    "1. **`from_json`**: Mengonversi string JSON menjadi kolom terstruktur sesuai **`transaction_schema`**.  \n",
    "2. **`col(\"value\")`**: Mengakses data mentah di stream.  \n",
    "3. **`alias(\"data\")` + `select(\"data.*\")`**: Ekstrak kolom seperti **`customer_id`** dan **`transaction_amount`** untuk pemrosesan lebih mudah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the JSON data\n",
    "transaction_stream_df = raw_stream_df \\\n",
    "    .select(from_json(col(\"value\"), transaction_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb704929",
   "metadata": {},
   "source": [
    "### 4️⃣ **Proses Data**\n",
    "\n",
    "Setelah data streaming dibaca, langkah selanjutnya adalah **mengolah data**. Pada contoh ini, kita ingin:  \n",
    "\n",
    "- **Grouping**: Mengelompokkan data berdasarkan `customer_id` agar setiap pelanggan dapat dianalisis.  \n",
    "- **Aggregation**:\n",
    "  - **`sum(\"transaction_amount\")`**: Menjumlahkan total transaksi setiap pelanggan.  \n",
    "  - **`alias(\"total_amount\")`**: Memberi nama baru pada kolom hasil agregasi agar lebih jelas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81033a0",
   "metadata": {},
   "source": [
    "**📊 Hitung Total Transaksi per Pelanggan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e391ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total transaction amount for each customer\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "customer_total_df = transaction_stream_df \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(sum(\"transaction_amount\").alias(\"total_amount\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c33177",
   "metadata": {},
   "source": [
    "### 5️⃣ **Tampilkan Hasil**\n",
    "\n",
    "Setelah data diproses, hasilnya ditulis ke **console** agar bisa dimonitor.  \n",
    "\n",
    "- **`writeStream`**: Menentukan tujuan keluaran data hasil proses.  \n",
    "  - **`outputMode(\"complete\")`**: Menampilkan hasil agregasi penuh setiap kali query dijalankan.  \n",
    "  - **`format(\"console\")`**: Menampilkan hasil di terminal.  \n",
    "- **`start()`**: Memulai proses streaming secara real-time.  \n",
    "- **`awaitTermination()`**: Menjaga aplikasi tetap berjalan hingga dihentikan manual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5947a37",
   "metadata": {},
   "source": [
    "**📤 Kode untuk Menulis Hasil ke Console**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad404b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the result to the console\n",
    "query = customer_total_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the termination of the query\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796102a3",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "  <center>Kode ini akan error apabila langsung dijalankan tanpa menjalankan socket `nc` di terminal</center>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a00dc",
   "metadata": {},
   "source": [
    "### 6️⃣ **Simulasi Data Transaksi Real-Time**\n",
    "\n",
    "Untuk menguji, gunakan **`nc` (netcat)** untuk mengirim data JSON ke socket.  \n",
    "1. Jalankan perintah berikut di terminal:  \n",
    "   ```bash\n",
    "   nc -lk 9999\n",
    "   ```  \n",
    "   - **`-l`**: Mode listening untuk menerima koneksi.  \n",
    "   - **`-k`**: Membuka koneksi terus-menerus.  \n",
    "\n",
    "2. Masukkan data transaksi dalam format JSON:  \n",
    "   ```json\n",
    "   {\"customer_id\": \"C001\", \"transaction_amount\": 100.50}\n",
    "   {\"customer_id\": \"C002\", \"transaction_amount\": 200.75}\n",
    "   {\"customer_id\": \"C001\", \"transaction_amount\": 150.25}\n",
    "   {\"customer_id\": \"C003\", \"transaction_amount\": 50.00}\n",
    "   {\"customer_id\": \"C002\", \"transaction_amount\": 100.00}\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812a494",
   "metadata": {},
   "source": [
    "**🖥️ Hasil di Console**\n",
    "\n",
    "Spark akan memproses data dan menampilkan hasil agregasi secara real-time:\n",
    "\n",
    "```plaintext\n",
    "+-----------+------------+\n",
    "|customer_id|total_amount|\n",
    "+-----------+------------+\n",
    "|       C001|      250.75|\n",
    "|       C002|      300.75|\n",
    "|       C003|       50.00|\n",
    "+-----------+------------+\n",
    "```\n",
    "\n",
    "\n",
    "**📌 Penjelasan Hasil**\n",
    "- **`C001`**: 100.50 + 150.25 = **250.75**  \n",
    "- **`C002`**: 200.75 + 100.00 = **300.75**  \n",
    "- **`C003`**: Hanya 50.00.\n",
    "\n",
    "✨ Dengan simulasi ini, Anda bisa melihat bagaimana data streaming diproses secara real-time oleh Spark! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e84d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert\">\n",
    "\n",
    "**Why Netcat? 🤔**\n",
    "\n",
    "🔹 **Simulasi Mudah**: Uji aliran data real-time tanpa sistem kompleks.  \n",
    "🔹 **Cocok untuk Pemula**: Belajar konsep streaming sebelum masuk ke tools produksi.\n",
    "\n",
    "**Untuk Produksi** \n",
    "\n",
    "Ganti **Netcat** dengan sistem yang lebih andal, seperti: \n",
    " \n",
    "- **Apache Kafka**  \n",
    "- **Amazon Kinesis**  \n",
    "- **Google Pub/Sub**\n",
    "</div> \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3facc78b",
   "metadata": {},
   "source": [
    "# 🚀 Enhancing Real-Time Streaming with Messaging Systems  \n",
    "\n",
    "Dalam dunia keuangan, seperti **monitoring transaksi**, **deteksi penipuan**, atau **analisis harga saham**, kita perlu alat andal untuk menangani data real-time. Di sinilah **Apache Kafka** hadir! Berbeda dengan server sederhana seperti `nc`, Kafka adalah platform terdistribusi yang dirancang khusus untuk mengelola data aliran besar secara efisien. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4bdb32",
   "metadata": {},
   "source": [
    "## 🧐 Apa itu Apache Kafka?  \n",
    "\n",
    "**Apache Kafka** adalah sistem pesan terdistribusi open-source untuk menangani aliran data secara real-time. Dibuat oleh LinkedIn, Kafka kini menjadi alat penting dalam industri, termasuk keuangan, untuk mengolah data dengan cepat dan skalabel.  \n",
    "\n",
    "### 🌟 *Kenapa Kafka Penting untuk Keuangan?*  \n",
    "Kafka membantu bisnis:  \n",
    "1. 💸 **Memproses Transaksi**: Tangani jutaan transaksi per detik dengan latensi rendah.  \n",
    "2. 🔒 **Deteksi Penipuan**: Identifikasi pola mencurigakan secara real-time.  \n",
    "3. 📈 **Analisis Saham**: Pantau data pasar langsung untuk keputusan cepat.  \n",
    "4. 👥 **Wawasan Pelanggan**: Personalisasi layanan berdasarkan data perilaku real-time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b59831",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🧩 Komponen Utama Kafka  \n",
    "\n",
    "Mari kita kenali bagian-bagian inti Kafka:  \n",
    "\n",
    "1. **📌 Topics**:  \n",
    "   - Saluran untuk pesan, seperti `\"transactions\"` atau `\"stock_prices\"`.  \n",
    "   - Semua pesan terkait disimpan di topik untuk akses real-time.  \n",
    "\n",
    "2. **✉️ Producers**:  \n",
    "   - Sistem yang mengirim pesan ke topik Kafka. Contoh: Platform trading mengirim data ke `\"transactions\"`.  \n",
    "\n",
    "3. **🛠️ Consumers**:  \n",
    "   - Aplikasi yang membaca pesan dari topik. Misalnya, aplikasi deteksi penipuan membaca dari `\"transactions\"`.  \n",
    "\n",
    "4. **🔗 Brokers**:  \n",
    "   - Server yang menyimpan dan mengelola pesan. Kafka mendistribusikan dan mereplikasi data untuk ketahanan.  \n",
    "\n",
    "5. **⚙️ Partitions**:  \n",
    "   - Membagi topik menjadi bagian lebih kecil, misalnya berdasarkan wilayah: `\"Jakarta\"`, `\"Bogor\"`.  \n",
    "\n",
    "6. **🔢 Offset**:  \n",
    "   - ID unik untuk setiap pesan dalam partisi. Konsumen melacak pesan dengan offset ini.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5197a5d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔄 Bagaimana Kafka Mengelola Data Real-Time?  \n",
    "\n",
    "1. **📤 Mengirim Data ke Kafka**  \n",
    "   - **Producers** mengirim pesan ke topik. Contoh:  \n",
    "     ```json  \n",
    "     {\"transaction_id\": \"T12345\", \"amount\": 500.0, \"currency\": \"USD\"}  \n",
    "     ```  \n",
    "\n",
    "2. **📂 Partisi dan Penyimpanan**  \n",
    "   - Kafka mempartisi data berdasarkan kunci (misalnya ID transaksi).  \n",
    "   - Pesan disimpan berurutan untuk proses real-time.  \n",
    "\n",
    "3. **📥 Mengonsumsi Data**  \n",
    "   - **Consumers** membaca pesan dari topik dan memprosesnya (e.g., model deteksi penipuan).  \n",
    "\n",
    "4. **⏳ Penyimpanan dan Retensi**  \n",
    "   - Kafka menyimpan pesan untuk durasi tertentu, penting untuk audit & kepatuhan.  \n",
    "\n",
    "5. **📈 Skalabilitas dan Ketahanan**  \n",
    "   - Kafka tetap cepat meski volume data besar. Jika broker gagal, replika mengambil alih otomatis.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d348ad8",
   "metadata": {},
   "source": [
    "## 📈 Finance Use Case: Real-Time Loan Monitoring with Kafka  \n",
    "\n",
    "### **📝 Scenario**  \n",
    "\n",
    "Sebuah bank ingin memproses aplikasi pinjaman secara **real-time** untuk memberikan keputusan cepat: **disetujui** atau **ditolak**. Dengan **Apache Kafka**, bank dapat meningkatkan efisiensi dan keandalan proses ini. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **🔄 Alur Proses**  \n",
    "\n",
    "![](assets/loan_applications_kafka.png)\n",
    "\n",
    "\n",
    "#### **1️⃣ Producer: Mengirim Data Aplikasi Pinjaman**  \n",
    "\n",
    "- **Apa yang dilakukan**:  \n",
    "  Aplikasi perbankan online mengirim data pelanggan (misalnya pendapatan, skor kredit, jumlah pinjaman) ke topik Kafka bernama `\"loan_applications\"`.  \n",
    "\n",
    "\n",
    "#### **2️⃣ Broker: Mengelola Aliran Data**  \n",
    "\n",
    "- **Peran Kafka**:  \n",
    "  - Membagi topik `\"loan_applications\"` menjadi partisi (misalnya berdasarkan wilayah).  \n",
    "  - Mereplikasi data untuk memastikan ketersediaan tinggi, meskipun server gagal.  \n",
    "  - Menyimpan data untuk durasi tertentu guna keperluan reprocessing.  \n",
    "- **Manfaat**:  \n",
    "  Kafka memastikan **tidak ada data yang hilang** dan memungkinkan proses paralel untuk keputusan cepat.  \n",
    "\n",
    "#### **3️⃣ Consumer: Prediksi Keputusan Pinjaman**  \n",
    "\n",
    "- **Peran Consumer**:  \n",
    "  - Membaca data aplikasi dari topik Kafka.  \n",
    "  - **Model pembelajaran mesin** menganalisis fitur seperti pendapatan dan skor kredit untuk memutuskan:  \n",
    "    - ✅ **Disetujui**: Skor kredit tinggi dan pendapatan memadai.  \n",
    "    - ❌ **Ditolak**: Skor kredit rendah atau jumlah pinjaman terlalu besar.  \n",
    "- **Output**: Hasil dikirim ke topik baru, namun pada pembelajaran kali ini kita hanya menampilkan pada jupyter notebook console \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0090491b",
   "metadata": {},
   "source": [
    "## ⚙️ Langkah Persiapan Kafka  \n",
    "\n",
    "### **🖥️ Instalasi di Mac**  \n",
    "\n",
    "1. **📥 Unduh Kafka**  \n",
    "   - Kunjungi [Apache Kafka Download Page](https://kafka.apache.org/downloads).  \n",
    "   - Unduh versi binari Kafka (misalnya `kafka_2.13-3.8.1.tgz`).  \n",
    "\n",
    "2. **🗂️ Ekstraksi File Kafka**  \n",
    "   - Ekstrak file ke direktori seperti `Tools/kafka`.  \n",
    "\n",
    "3. **🔧 Konfigurasi Direktori Data Kafka**  \n",
    "   - Buat folder `data` di dalam direktori Kafka.  \n",
    "   - Edit file `config/kraft/server.properties` dan ubah:  \n",
    "     ```properties\n",
    "     log.dirs=data\n",
    "     ```\n",
    "   - Simpan perubahan.  \n",
    "\n",
    "4. **🆔 Format Penyimpanan Kafka**  \n",
    "   - Hasilkan UUID untuk cluster:  \n",
    "     ```bash\n",
    "     ./bin/kafka-storage.sh random-uuid\n",
    "     ```  \n",
    "   - Gunakan UUID ini untuk memformat penyimpanan Kafka:  \n",
    "     ```bash\n",
    "     ./bin/kafka-storage.sh format --cluster-id <UUID> --config config/kraft/server.properties\n",
    "     ```  \n",
    "\n",
    "5. **▶️ Menjalankan Kafka**  \n",
    "   - Mulai Kafka:  \n",
    "     ```bash\n",
    "     ./bin/kafka-server-start.sh config/kraft/server.properties\n",
    "     ```  \n",
    "   - Hentikan Kafka dengan `CTRL+C`.  \n",
    "\n",
    "---\n",
    "\n",
    "### **🖥️ Instalasi di Windows**  \n",
    "\n",
    "1. **📥 Unduh Kafka**  \n",
    "   - Unduh file Kafka dan ekstrak menggunakan **WinRAR** atau alat serupa.  \n",
    "   - Rename folder ke `kafka` untuk kemudahan.  \n",
    "\n",
    "2. **🔧 Konfigurasi ZooKeeper dan Kafka**  \n",
    "   - Ubah lokasi penyimpanan data di `config/zookeeper.properties` dan `config/kraft/server.properties`:  \n",
    "     ```properties\n",
    "     dataDir=data/zookeeper\n",
    "     log.dirs=data/kafka-logs\n",
    "     ```  \n",
    "\n",
    "3. **🆔 Format Penyimpanan Kafka**  \n",
    "   - Hasilkan UUID dengan:  \n",
    "     ```bash\n",
    "     bin\\windows\\kafka-storage.bat random-uuid\n",
    "     ```  \n",
    "   - Format penyimpanan Kafka:  \n",
    "     ```bash\n",
    "     bin\\windows\\kafka-storage.bat format --cluster-id <UUID> --config config\\kraft\\server.properties\n",
    "     ```  \n",
    "\n",
    "4. **▶️ Menjalankan Kafka**  \n",
    "   - Jalankan ZooKeeper:  \n",
    "     ```bash\n",
    "     bin\\windows\\zookeeper-server-start.bat config\\zookeeper.properties\n",
    "     ```  \n",
    "   - Jalankan Kafka:  \n",
    "     ```bash\n",
    "     bin\\windows\\kafka-server-start.bat config\\kraft\\server.properties\n",
    "     ```  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a287d7c",
   "metadata": {},
   "source": [
    "## Step 2: Membuat Kafka Producer\n",
    "\n",
    "![](assets/st_UI_producer.png)\n",
    "\n",
    "Kafka producer digunakan untuk mengirim pesan ke topik Kafka agar dapat diproses lebih lanjut. Dalam langkah ini, kita akan:  \n",
    "\n",
    "1. Mengkonfigurasi Kafka producer menggunakan pustaka `confluent_kafka`.  \n",
    "2. Membangun antarmuka pengguna (UI) dengan Streamlit untuk input data aplikasi pinjaman.  \n",
    "3. Mengirim data yang dikumpulkan ke topik Kafka bernama `\"loan_applications\"`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f6c7b",
   "metadata": {},
   "source": [
    "### **1️⃣ Kode Kafka Producer**\n",
    "\n",
    "Berikut adalah kode Python untuk mengatur Kafka producer:\n",
    "\n",
    "\n",
    "```python\n",
    "from confluent_kafka import Producer\n",
    "import json\n",
    "\n",
    "# Konfigurasi Kafka producer\n",
    "producer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092'  # Alamat Kafka broker\n",
    "}\n",
    "\n",
    "# Membuat instance Kafka producer\n",
    "producer = Producer(producer_config)\n",
    "\n",
    "def kirim_pesan(topik, nilai):\n",
    "    \"\"\"\n",
    "    Mengirim pesan ke topik Kafka yang ditentukan.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Kirim data ke topik Kafka dalam format JSON\n",
    "        producer.produce(topik, value=json.dumps(nilai))\n",
    "        producer.flush()  # Pastikan pesan terkirim\n",
    "        print(f\"Pesan berhasil dikirim ke topik '{topik}': {nilai}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Kesalahan saat mengirim pesan: {e}\")\n",
    "```\n",
    "\n",
    "**🔑 Poin Penting**:  \n",
    "\n",
    "- **`bootstrap.servers`**: Alamat Kafka broker. Secara default, menggunakan `localhost:9092` untuk pengaturan lokal.  \n",
    "- **`produce()`**: Mengirim data ke topik Kafka yang ditentukan.  \n",
    "- **`flush()`**: Memastikan semua pesan yang tersimpan dalam buffer terkirim sebelum melanjutkan proses.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a0dc9",
   "metadata": {},
   "source": [
    "### **2️⃣ Membuat Antarmuka dengan Streamlit**\n",
    "\n",
    "Berikut adalah cara membuat antarmuka web interaktif menggunakan Streamlit untuk pengumpulan data aplikasi pinjaman:\n",
    "\n",
    "```python\n",
    "# Streamlit UI for loan applications\n",
    "st.title(\"Credit Risk Prediction\")\n",
    "\n",
    "with st.form(\"credit_risk_form\"):\n",
    "    person_home_ownership = st.selectbox(\"Home Ownership\", [\"RENT\", \"OWN\", \"MORTGAGE\"])\n",
    "    loan_intent = st.selectbox(\"Loan Intent\", [\"PERSONAL\", \"EDUCATION\", \"DEBT CONSOLIDATION\"])\n",
    "    loan_grade = st.selectbox(\"Loan Grade\", [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"])\n",
    "    person_age = st.number_input(\"Age\", min_value=0, step=1)\n",
    "    person_income = st.number_input(\"Income\", min_value=0.0, step=0.1)\n",
    "    person_emp_length = st.number_input(\"Employment Length (in years)\", min_value=0, step=1)\n",
    "    loan_amnt = st.number_input(\"Loan Amount\", min_value=0.0, step=0.1)\n",
    "    loan_int_rate = st.number_input(\"Interest Rate\", min_value=0.0, step=0.1)\n",
    "    cb_person_cred_hist_length = st.number_input(\"Credit History Length (in years)\", min_value=0, step=1)\n",
    "\n",
    "    submitted = st.form_submit_button(\"Submit\")\n",
    "\n",
    "# Menangani pengiriman formulir\n",
    "if submitted:\n",
    "    # Mengumpulkan data dari formulir ke dalam dictionary\n",
    "    data_pinjaman = {\n",
    "        \"person_home_ownership\": person_home_ownership,\n",
    "        \"loan_intent\": loan_intent,\n",
    "        \"loan_grade\": loan_grade,\n",
    "        \"person_age\": person_age,\n",
    "        \"person_income\": person_income,\n",
    "        \"person_emp_length\": person_emp_length,\n",
    "        \"loan_amnt\": loan_amnt,\n",
    "        \"loan_int_rate\": loan_int_rate,\n",
    "        \"cb_person_cred_hist_length\": cb_person_cred_hist_length\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Mengirim data ke Kafka\n",
    "        kirim_pesan(\"loan_applications\", data_pinjaman)\n",
    "        st.success(\"✅ Aplikasi pinjaman berhasil dikirim ke Kafka!\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"❌ Kesalahan saat mengirim data ke Kafka: {e}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30851f9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### **How This Works:**\n",
    "\n",
    "- **Input Pengguna**:  \n",
    "  Pengguna memasukkan detail pinjaman, seperti **usia**, **pendapatan**, **jumlah pinjaman**, dll., ke dalam formulir.  \n",
    "\n",
    "- **Transformasi Data**:  \n",
    "  Data dari formulir dikumpulkan ke dalam dictionary dan diubah menjadi format JSON sebelum dikirim ke Kafka.  \n",
    "\n",
    "- **Pengiriman Pesan**:  \n",
    "  Kafka producer mempublikasikan pesan JSON ke topik `\"loan_applications\"`.  \n",
    "\n",
    "\n",
    "### 3️⃣ **Menjalankan Aplikasi Streamlit**\n",
    "\n",
    "1. Simpan kode ke file, misalnya `streamlit_kafka_producer.py`.  \n",
    "2. Jalankan aplikasi Streamlit:  \n",
    "   ```bash\n",
    "   streamlit run streamlit_kafka_producer.py\n",
    "   ```  \n",
    "3. Buka browser (Streamlit akan menampilkan tautannya di terminal). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b5a2d2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><br>\n",
    "  <center><h2>Read Data From Kafka</h2></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b27da5b",
   "metadata": {},
   "source": [
    "## Step 3: Kafka Consumer untuk Membaca Data dan Mengubahnya ke DataFrame\n",
    "\n",
    "### Membaca Data dari Kafka\n",
    "\n",
    "Pada langkah ini, kita akan membaca pesan yang dikirim oleh Kafka Producer dan mengubahnya ke format terstruktur (seperti DataFrame). Untuk itu, kita akan menggunakan **PySpark** sebagai alat untuk memproses dan menganalisis data Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed5520",
   "metadata": {},
   "source": [
    "**1️⃣ Mengatur PySpark dengan Kafka**\n",
    "\n",
    "Langkah pertama adalah membuat **Spark session** yang memungkinkan kita berinteraksi dengan Spark dan Kafka.\n",
    "\n",
    "Namun ada beberapa configurasi yang peru kita atur:\n",
    "\n",
    "- **`spark.jars.packages`**: Menentukan pustaka Kafka dan Spark SQL yang diperlukan untuk membaca data dari Kafka.\n",
    "- **`master(\"local[*]\")`**: Memberitahu Spark untuk berjalan secara lokal dengan memanfaatkan semua core CPU yang tersedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce2b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Credit Data Kafka\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,org.apache.kafka:kafka-clients:3.8.1\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()  # Create the session\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f30a7e6",
   "metadata": {},
   "source": [
    "**2️⃣ Membaca Data dari Kafka**\n",
    "\n",
    "Setelah Spark session dibuat, kita bisa membaca data dari topik Kafka menggunakan fungsi `spark.read.format(\"kafka\")`.\n",
    "\n",
    "Ada beberapa `option` (pengaturan) yang perlu didefinisikan:\n",
    "\n",
    "- **`kafka.bootstrap.servers`**: Alamat Kafka broker, dalam hal ini `localhost:9092` untuk server lokal.\n",
    "- **`subscribe`**: Nama topik Kafka yang akan dibaca, misalnya `loan_applications`.\n",
    "- **`startingOffsets`**: Mengatur agar pembacaan dimulai dari data awal (earliest) yang tersedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"kafka\")  # Gunakan Kafka sebagai sumber data\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")  # Alamat Kafka broker\n",
    "    .option(\"subscribe\", \"loan_applications\")  # Nama topik Kafka\n",
    "    .option(\"startingOffsets\", \"earliest\")  # Mulai membaca dari offset paling awal\n",
    "    .load()  # Memuat data ke dalam DataFrame\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f536c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan struktur data\n",
    "kafka_df.printSchema()  # Print schema to see the structure of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2263bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df.show()  # Display the first few rows of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7783ef",
   "metadata": {},
   "source": [
    "\n",
    "**3️⃣ Mendefinisikan Skema untuk Data**\n",
    "\n",
    "Pesan dari Kafka disimpan dalam format **binary**, sehingga kita perlu mengubahnya menjadi format terstruktur. Untuk itu, kita mendefinisikan **schema** yang sesuai dengan data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22eac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "# Define the schema for the data inside the 'value' column\n",
    "schema = StructType([\n",
    "    StructField(\"person_home_ownership\", StringType(), True),\n",
    "    StructField(\"loan_intent\", StringType(), True),\n",
    "    StructField(\"loan_grade\", StringType(), True),\n",
    "    StructField(\"person_age\", IntegerType(), True),\n",
    "    StructField(\"person_income\", FloatType(), True),\n",
    "    StructField(\"person_emp_length\", IntegerType(), True),\n",
    "    StructField(\"loan_amnt\", FloatType(), True),\n",
    "    StructField(\"loan_int_rate\", FloatType(), True),\n",
    "    StructField(\"cb_person_cred_hist_length\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef1967",
   "metadata": {},
   "source": [
    "- **`StringType`** digunakan untuk data teks (misalnya `person_home_ownership`).\n",
    "- **`IntegerType`** dan **`FloatType`** digunakan untuk data numerik (misalnya `person_age`, `loan_amnt`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6fef17",
   "metadata": {},
   "source": [
    "**4️⃣ Mengubah Pesan Kafka ke Format String dan Parsing JSON**\n",
    "\n",
    "Pesan Kafka dalam kolom **`value`** perlu diubah dari format binary menjadi string, lalu diparsing ke format JSON menggunakan schema yang sudah didefinisikan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ubah 'value' dari binary ke string dan parsing ke format JSON\n",
    "\n",
    "# Konversi binary ke string\n",
    "df = kafka_df.withColumn(\"value_str\", col(\"value\").cast(\"string\"))\n",
    "\n",
    "# Parsing JSON berdasarkan schema\n",
    "df_parsed = df.withColumn(\"data\", from_json(col(\"value_str\"), schema)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c3aa6",
   "metadata": {},
   "source": [
    "- **`col(\"value\").cast(\"string\")`**: Mengonversi data Kafka dari format binary ke string.\n",
    "- **`from_json(col(\"value_str\"), schema)`**: Memparsing string JSON ke DataFrame terstruktur berdasarkan schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada6c4be",
   "metadata": {},
   "source": [
    "**5️⃣ Memilih dan Menampilkan Data**\n",
    "\n",
    "Setelah parsing, kita dapat memilih kolom yang relevan dan menampilkan hasilnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memilih kolom yang relevan\n",
    "df_result = df_parsed.select(\n",
    "    \"data.*\",  # Semua kolom dari data yang diparsing\n",
    "    \"key\", \"topic\", \"partition\", \"offset\", \"timestamp\", \"timestampType\"  # Metadata Kafka\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0954c",
   "metadata": {},
   "source": [
    "- **`select(\"data.*\", ...)`**: This extracts all the fields from the parsed JSON data and adds Kafka metadata (like `key`, `topic`, `partition`, etc.).\n",
    "- **`df_result.show()`**: This displays the DataFrame content, allowing you to view the structured data from Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3957be74",
   "metadata": {},
   "source": [
    "**6️⃣ Hasil Akhir**\n",
    "\n",
    "Setelah kode di atas dijalankan, data yang diterima dari Kafka akan tampil dalam format terstruktur seperti berikut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83235bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan hasil\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79fa95",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\"><br>\n",
    "  <center><h2>Streaming Data From Kafka</h2></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd5d9e",
   "metadata": {},
   "source": [
    "## Streaming Data dari Kafka\n",
    "\n",
    "Pada langkah ini, kita akan menggunakan data **streaming** dari Kafka, yang memungkinkan kita membaca data secara real-time dan memprosesnya secara langsung. Tujuannya adalah membaca data dari topik Kafka dan mengubahnya menjadi format terstruktur menggunakan **PySpark**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156cf8b",
   "metadata": {},
   "source": [
    "**1️⃣ Mengatur Spark Session**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Credit Data Kafka\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,org.apache.kafka:kafka-clients:3.8.1\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()  # Create the session\n",
    "\n",
    "spark  # Display the Spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3f52e",
   "metadata": {},
   "source": [
    "**2️⃣ Membaca Data Streaming dari Kafka**\n",
    "\n",
    "Setelah Spark session dibuat, kita dapat menggunakan fungsi **`readStream`** untuk terus membaca pesan dari topik Kafka secara real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4462f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")  # Membaca data dari Kafka\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")  # Alamat server Kafka\n",
    "    .option(\"subscribe\", \"loan_applications\")  # Nama topik Kafka\n",
    "    .option(\"startingOffsets\", \"earliest\")  # Mulai membaca dari data awal\n",
    "    .load()  # Memuat data ke dalam DataFrame\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165d69c",
   "metadata": {},
   "source": [
    "**3️⃣ Mendefinisikan Skema untuk Data**\n",
    "\n",
    "Pesan dari Kafka biasanya berupa format **binary**, sehingga kita perlu mendefinisikan skema untuk mengubah data mentah menjadi format terstruktur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff841a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "# Definisi skema untuk kolom 'value'\n",
    "schema = StructType([\n",
    "    StructField(\"person_home_ownership\", StringType(), True),\n",
    "    StructField(\"loan_intent\", StringType(), True),\n",
    "    StructField(\"loan_grade\", StringType(), True),\n",
    "    StructField(\"person_age\", IntegerType(), True),\n",
    "    StructField(\"person_income\", FloatType(), True),\n",
    "    StructField(\"person_emp_length\", IntegerType(), True),\n",
    "    StructField(\"loan_amnt\", FloatType(), True),\n",
    "    StructField(\"loan_int_rate\", FloatType(), True),\n",
    "    StructField(\"cb_person_cred_hist_length\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132ed708",
   "metadata": {},
   "source": [
    "**4️⃣ Parsing Data dari Format Binary ke JSON**\n",
    "\n",
    "Setelah skema didefinisikan, kita dapat mengonversi kolom **`value`** (yang berisi data binary) menjadi string yang dapat dibaca. Kemudian, kita parsing data JSON menggunakan skema yang sudah dibuat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ubah 'value' dari binary ke string, lalu parsing ke format JSON\n",
    "df_parsed = stream_kafka_df.withColumn(\"value_str\", col(\"value\").cast(\"string\"))\n",
    "df_data = df_parsed.withColumn(\"data\", from_json(col(\"value_str\"), schema))\n",
    "\n",
    "# Memilih kolom yang diinginkan\n",
    "df_result = df_data.select(\n",
    "    \"data.*\",  # Semua kolom dari data yang diparsing\n",
    "    \"key\", \"topic\", \"partition\", \"offset\", \"timestamp\", \"timestampType\"  # Metadata Kafka\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacffab",
   "metadata": {},
   "source": [
    "**5️⃣ Menampilkan Data Secara Streaming**\n",
    "\n",
    "Sekarang, kita akan menampilkan hasil streaming. Karena ini adalah operasi **streaming**, data akan terus diproses dan ditampilkan secara real-time.\n",
    "\n",
    "Beberapa parameter yang bisa digunakan untuk streaming data:\n",
    "\n",
    "- **`outputMode(\"append\")`**: Mode ini menambahkan baris baru ke output tanpa mengubah data yang sudah ada.\n",
    "- **`format(\"console\")`**: Menulis output stream ke konsol. Alternatif lain adalah `\"memory\"` untuk menyimpan data di memori.\n",
    "- **`awaitTermination()`**: Menjaga query tetap berjalan tanpa batas waktu, menunggu data baru untuk diproses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a3939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tulis hasil stream ke konsol dalam mode append\n",
    "query = (\n",
    "    df_result\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")  # Menambahkan data baru ke output\n",
    "    .format(\"console\")  # Menampilkan hasil di konsol (bisa juga 'memory' untuk query di memori)\n",
    "    .start()  # Memulai streaming query\n",
    ")\n",
    "\n",
    "# Menunggu stream berjalan\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044dbc50",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert\">\n",
    "<p>\n",
    "\n",
    "Pada langkah ini, kita telah membuat **Kafka consumer** yang membaca data secara terus-menerus dari topik Kafka. Dengan **PySpark**, kita berhasil:\n",
    "1. Membaca data streaming dari Kafka.\n",
    "2. Memparsing pesan JSON dari format binary menjadi terstruktur.\n",
    "3. Menampilkan hasil data secara real-time di konsol.\n",
    "</p>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26cd22",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\"><br>\n",
    "  <center><h2>Streaming Data From Kafka + PySpark for Prediction</h2></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e85a94",
   "metadata": {},
   "source": [
    "## Streaming Data dari Kafka + PySpark untuk Prediksi\n",
    "\n",
    "Pada bagian ini kita akan menerapkan **model machine learning** yang telah dilatih sebelumnya dalam skenario streaming, dengan memanfaatkan **Kafka** untuk menerima data secara real-time, **PySpark** untuk pemrosesan data, dan **model prediksi** untuk memberikan hasil secara langsung. Berikut langkah-langkahnya:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38424a0c",
   "metadata": {},
   "source": [
    "**1️⃣ Membuat Spark Session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Membuat Spark session untuk menghubungkan ke cluster Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Credit Data Kafka Predict\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,org.apache.kafka:kafka-clients:3.8.1\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4)\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f00abb",
   "metadata": {},
   "source": [
    "**2️⃣ Mendefinisikan Skema untuk Data Masuk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16cccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "# Define schema for the data in Kafka messages\n",
    "schema = StructType([\n",
    "    StructField(\"person_home_ownership\", StringType(), True),\n",
    "    StructField(\"loan_intent\", StringType(), True),\n",
    "    StructField(\"loan_grade\", StringType(), True),\n",
    "    StructField(\"person_age\", IntegerType(), True),\n",
    "    StructField(\"person_income\", FloatType(), True),\n",
    "    StructField(\"person_emp_length\", IntegerType(), True),\n",
    "    StructField(\"loan_amnt\", FloatType(), True),\n",
    "    StructField(\"loan_int_rate\", FloatType(), True),\n",
    "    StructField(\"cb_person_cred_hist_length\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9333fe",
   "metadata": {},
   "source": [
    "**3️⃣ Membaca Data Streaming dari Kafka**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa757334",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_kafka_df_predict = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"loan_applications\")\n",
    "    .option(\"startingOffsets\", \"earliest\")  # Start reading from the earliest message\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1047fcbe",
   "metadata": {},
   "source": [
    "**4️⃣ Memuat Model yang Telah Dilatih**\n",
    "\n",
    "Kita akan memuat **model pre-trained** yang sebelumnya telah dilatih menggunakan data historis. Model ini akan digunakan untuk memprediksi hasil aplikasi pinjaman baru yang diterima melalui Kafka.\n",
    "\n",
    "Model yang dimuat:\n",
    "\n",
    "- **RandomForestClassificationModel**: Model untuk memprediksi apakah aplikasi pinjaman akan disetujui atau ditolak.\n",
    "- **MinMaxScalerModel**: Model untuk **scaling** fitur ke dalam rentang standar.\n",
    "- **PipelineModel**: Mengelola preprocessing seperti encoding, pengisian nilai yang hilang, dan transformasi fitur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a663a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.ml.feature import MinMaxScalerModel, VectorAssembler\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Load pre-trained models\n",
    "rf_model = RandomForestClassificationModel.load(\"models_new1/random_forest_credit_risk\")\n",
    "scaler_model = MinMaxScalerModel.load(\"models_new1/minmax_scaler\")\n",
    "pipeline_model = PipelineModel.load(\"models_new1/preprocessing_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5c2d2",
   "metadata": {},
   "source": [
    "**5️⃣ Parsing dan Preprocessing Data**\n",
    "\n",
    "Pesan dari Kafka berbentuk **binary**, sehingga kita perlu mengubahnya ke string, parsing JSON, lalu menyesuaikan dengan skema yang sudah dibuat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a601fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "\n",
    "# Parsing JSON dari kolom 'value' Kafka\n",
    "df_parsed = stream_kafka_df_predict.withColumn(\"value_str\", col(\"value\").cast(\"string\"))\n",
    "df_data = df_parsed.withColumn(\"data\", from_json(col(\"value_str\"), schema)).select(\"data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f7b9a",
   "metadata": {},
   "source": [
    "**6️⃣ Definisi Logika Prediksi dan Penerapan Pipeline**\n",
    "\n",
    "Langkah ini mendefinisikan logika untuk memproses data streaming dan menggunakan model yang sudah dilatih untuk prediksi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define function to apply the prediction and add the result column\n",
    "def predict_and_add_column(input_data):\n",
    "    # Apply preprocessing pipeline (e.g., encoding categorical variables)\n",
    "    processed_data = pipeline_model.transform(input_data)\n",
    "    \n",
    "    # Assemble features into a single vector column\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=['person_age', 'person_income', 'person_emp_length', 'loan_amnt', 'loan_int_rate',\n",
    "                   'cb_person_cred_hist_length', 'person_home_ownership_encoded', 'loan_intent_encoded', 'loan_grade_encoded'],\n",
    "        outputCol=\"features_unscaled\"\n",
    "    )\n",
    "    assembled_data = assembler.transform(processed_data)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaled_data = scaler_model.transform(assembled_data)\n",
    "    \n",
    "    # Predict the loan approval (1 = Approved, 0 = Denied)\n",
    "    predictions = rf_model.transform(scaled_data)\n",
    "    \n",
    "    # Add 'approval_status' column based on prediction\n",
    "    result_data = predictions.withColumn(\n",
    "        \"approval_status\",\n",
    "        F.when(col(\"prediction\") == 1, \"Approved\").otherwise(\"Denied\")\n",
    "    )\n",
    "    \n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a14d16",
   "metadata": {},
   "source": [
    "<div class=\"alert\">\n",
    "  <details>\n",
    "    <summary><b>Penjelasan</b></summary> \n",
    "    \n",
    "**1. Preprocessing Data**\n",
    "Pertama-tama, kita menggunakan **pipeline model** yang sudah dilatih untuk memproses data yang masuk. Pipeline ini memastikan bahwa fitur kategori di-encode dengan benar dan data ditransformasikan ke dalam format yang dibutuhkan oleh model machine learning.\n",
    "\n",
    "- **Preprocessing** di pipeline model dapat mencakup berbagai langkah, seperti mengisi nilai yang hilang, encoding variabel kategorikal (misalnya, `person_home_ownership`), dan mengubah format data agar sesuai untuk input model.\n",
    "\n",
    "**2. Menyusun Fitur**\n",
    "Selanjutnya, kita menyusun semua fitur individu menjadi satu vektor fitur yang tunggal. Proses ini dilakukan dengan menggunakan **VectorAssembler**, yang menggabungkan kolom-kolom fitur menjadi satu kolom vektor. Ini adalah langkah penting karena banyak model machine learning (termasuk Random Forest) membutuhkan input dalam bentuk vektor fitur.\n",
    "\n",
    "**3. Scaling Fitur**\n",
    "Fitur-fitur yang telah disusun dalam vektor kemudian akan **diskalakan** menggunakan **MinMaxScaler**. Scaling bertujuan untuk menormalkan nilai-nilai fitur sehingga berada dalam rentang yang sama. Hal ini penting karena banyak model machine learning, termasuk Random Forest, lebih efektif bila fitur-fitur memiliki skala yang seragam.\n",
    "\n",
    "**4. Prediksi dengan Model Random Forest**\n",
    "Setelah fitur-fitur diskalakan, kita mengaplikasikan model **RandomForest** yang telah dilatih sebelumnya untuk melakukan prediksi status persetujuan pinjaman. Model ini akan menghasilkan prediksi dalam bentuk nilai numerik, yaitu `1` untuk disetujui (approved) dan `0` untuk ditolak (denied).\n",
    "\n",
    "**5. Menambahkan Kolom Prediksi**\n",
    "Setelah prediksi dihasilkan, kita menambahkan kolom baru, yaitu **approval_status**, yang berisi status persetujuan pinjaman berdasarkan hasil prediksi. Jika prediksi bernilai `1`, maka kolom `approval_status` akan berisi \"Approved\", dan jika bernilai `0`, kolom tersebut akan berisi \"Denied\".\n",
    "\n",
    "    \n",
    "</details>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f4267",
   "metadata": {},
   "source": [
    "**7️⃣ Streaming Prediksi dan Menampilkan Hasil**\n",
    "\n",
    "Setelah fungsi prediksi diterapkan pada data streaming, hasil prediksi akan ditampilkan secara real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a6c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply prediction logic on the streaming data\n",
    "predicted_stream = predict_and_add_column(df_data)\n",
    "\n",
    "# Output the results to the console (for debugging or display in Jupyter notebook)\n",
    "query = (\n",
    "    predicted_stream\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")  # Keep adding new predictions as data comes in\n",
    "    .format(\"console\")  # Display results in console\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Wait for the stream to finish\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ba9900",
   "metadata": {},
   "source": [
    "<div class=\"alert\">\n",
    "<p>\n",
    "\n",
    "Langkah ini mengintegrasikan Kafka, PySpark, dan model machine learning untuk memproses dan memprediksi data real-time. **Manfaatnya**:\n",
    "1. **Real-time Processing**: Prediksi dilakukan segera setelah data diterima.\n",
    "2. **Scalable Architecture**: Spark dan Kafka memungkinkan pengolahan data dalam skala besar.\n",
    "3. **Efisiensi Model**: Memanfaatkan pipeline dan model pre-trained untuk alur kerja yang konsisten.\n",
    "\n",
    "</p>\n",
    "</div> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kafka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
